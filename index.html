<!DOCTYPE html>

<html>
<head>
  <meta charset="utf-8" />
  <title>Eric Wallace &mdash; Home</title>
  <link rel="stylesheet" href="master.css" />
  <link href="https://fonts.googleapis.com/css?family=Roboto+Slab:300,400,700" rel="stylesheet">

  <script>
      function unhide(divID) {
          var item = document.getElementById(divID);
          if (item) {
              item.className=(item.className=='hidden')?'unhidden':'hidden';
          }
  }
  </script>

  <script type="text/javascript">
    var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
  </script> 
  
  
    <link rel="icon" href="http://nlp.cs.berkeley.edu/logo.png" />
    <meta name="author" content="Eric Wallace">
    <meta name="keywords" content="Eric Wallace Berkeley AI2">
    <meta name="robots" content="index,follow">
    <meta name="description" content="Homepage of Eric Wallace Berkeley">
  
</head>

<body>
<div class="wrapper">

  <div class="posts-wfrapper">
    <div class="post">
      <img src='crop.gif' style="float:right; width:250px;"/>

      <h1>Eric Wallace</h1>

      <h2>ericwallace@berkeley.edu // <a style="font-size: 0.95em; font-weight:700" href="https://scholar.google.com/citations?user=SgST3LkAAAAJ" target="_blank"> Scholar </a> // <a style="font-size: 0.95em; font-weight:700" href="https://www.github.com/Eric-Wallace" target="_blank">GitHub</a> // <a style="font-size: 0.95em; font-weight:700" href="https://www.twitter.com/Eric_Wallace_" target="_blank">Twitter</a> // <a style="font-size: 0.95em; font-weight:700" href="CV.pdf" target="_blank">CV</a></h2>
      <br>
      <br>

            <p>I am a first-year PhD student at UC Berkeley advised by <a href="https://people.eecs.berkeley.edu/~klein/" target="_blank">Dan Klein</a> and <a href="https://people.eecs.berkeley.edu/~dawnsong/" target="_blank">Dawn Song</a>. I work on Machine Learning and Natural Language Processing as part of <a href="http://nlp.cs.berkeley.edu/" target="_blank">Berkeley NLP</a>, the <a href="https://rise.cs.berkeley.edu/" target="_blank">RISE Lab</a>, and Berkeley AI Research <a href="https://bair.berkeley.edu" target="_blank">(BAIR)</a>. 
            <br> <br>

            Before this, I did my undergrad at the University of Maryland, where I worked with <a href="http://www.umiacs.umd.edu/~jbg/" target="_blank">Jordan Boyd-Graber</a>. I spent most of 2019 working at the <a href="https://allenai.org/" target="_blank">Allen Institute for AI</a> with <a href="https://allenai.org/team/mattg/" target="_blank">Matt Gardner</a> and <a href="http://sameersingh.org/" target="_blank">Sameer Singh</a>.
            <br><br>

            <h3 style="margin-bottom:0.75em;">Research</h3>

            <p><b>Robustness</b> We study how an adversary or a <a href="https://arxiv.org/abs/2004.06100" target="_blank" style="font-size: 0.95em">distribution shift</a> can affect model behavior. We have created an adversarial attack called <a href="https://arxiv.org/abs/1908.07125" target="_blank" style="font-size: 0.95em">Universal Adversarial Triggers</a> that exposes egregious failures of state-of-the-art NLP systems. We have also used adversarial attacks to reveal counterintuitive model behavior on <a href="https://arxiv.org/abs/1804.07781" target="_blank" style="font-size: 0.95em">reduced</a> and  <a href="https://arxiv.org/abs/1809.02701" target="_blank" style="font-size: 0.95em">human-modified</a> inputs. Our current research works to increase model robustness and studies new attack vectors such as model stealing and data poisoning.</p>

            <p><b>Interpretability</b> We look to open up the black box of machine learning by interpreting model predictions. We have analyzed the <a href="https://arxiv.org/abs/1804.07781" target="_blank" style="font-size: 0.95em">limitations</a> of existing interpretation methods and designed new techniques for generating <a href="https://arxiv.org/abs/1902.00407" target="_blank" style="font-size: 0.95em">saliency</a> <a href="https://arxiv.org/abs/1809.02847" target="_blank" style="font-size: 0.95em">maps</a>. We facilitate research and adoption of interpretation methods through an <a href="https://arxiv.org/abs/1909.09251" target="_blank">open-source interpretation toolkit</a>. Our current research probes the <a href="https://arxiv.org/abs/1909.07940" target="_blank" style="font-size: 0.95em">naturally emergent knowledge</a> of pre-trained NLP models and studies when interpretations are useful in practice.</p>

            <p><b>Dataset Biases</b> We investigate how models can use subtle dataset patterns (<i>annotation artifacts</i>) to achieve high test accuracy without truly understanding a task. We have analyzed <a href="https://arxiv.org/abs/1905.05778" target="_blank" style="font-size: 0.95em">techniques</a> for discovering these artifacts and applied them to identify issues in existing <a href="https://arxiv.org/abs/1906.02900" target="_blank" style="font-size: 0.95em">datasets</a>. We also study how overreliance on dataset biases can cause downstream failures such as susceptibility to <a href="https://arxiv.org/abs/1908.07125" target="_blank" style="font-size: 0.95em">adversarial attacks</a>. Our current research creates new ways to evaluate models <a href="https://arxiv.org/abs/2004.02709" target="_blank" style="font-size: 0.95em">more comprehensively</a>.
    </p>
          <br>
      </div>
  </div>

  <div class="posts-wrapper" style="clear:both">
    <h3 style="margin-bottom:0.75em;">publications</h3>
    </i>
    <p>
    <ul class="pubs">
                
     
    <li>        
    <a href="https://arxiv.org/abs/2004.06100" target="_blank" style="color:black;font-size:1.0em">Pretrained Transformers Improve Out-of-Distribution Robustness</a><br>
    Dan Hendrycks*, Xiaoyuan Liu*, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song<br>
    <i>ACL 2020</i><br>
    How does pretraining affect <i>out-of-distribution</i> robustness? We create an OOD benchmark and use it to show that pretrained transformers such as BERT have substantially higher OOD accuracy and OOD detection rates compared to traditional NLP models.  
    <br> <a href="https://arxiv.org/abs/2004.06100" target="_blank">Paper</a> | <a href="https://twitter.com/Eric_Wallace_/status/1250507707674578944" target="_blank">Twitter</a> | <a href="javascript:unhide('robust20');">Citation</a>
  <div id="robust20" class="hidden">
  <pre>@inproceedings{hendrycks2020pretrained,
    Author = {Dan Hendrycks and Xiaoyuan Liu and Eric Wallace and Adam Dziedzic Rishabh Krishnan and Dawn Song},
    Booktitle = {Association for Computational Linguistics},                            
    Year = {2020},
    Title = {Pretrained Transformers Improve Out-of-Distribution Robustness}}
   </pre>
   </div>   
   </li>
        
    <li>        
            <a href="https://arxiv.org/abs/2002.11794" target="_blank" style="color:black;font-size:1.0em">
            Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers</a><br>
            Zhuohan Li*, Eric Wallace*, Sheng Shen*, Kevin Lin*, Kurt Keutzer, Dan Klein, and Joseph E. Gonzalez<br>
            <i>arXiv Preprint 2020</i><br>
            We show that <i>increasing</i> model size actually speeds up training and inference for Transformer models. The key idea is that you should use a very big model but do very few epochs and apply heavy compression.
            <br> <a href="https://bair.berkeley.edu/blog/2020/03/05/compress/" target="_blank">Blog</a> | <a href="https://twitter.com/Eric_Wallace_/status/1235616760595791872" target="_blank">Twitter</a> | <a href="https://arxiv.org/abs/2002.11794" target="_blank">Paper</a> | <a href="javascript:unhide('efficient20');">Citation</a>                
              <div id="efficient20" class="hidden">
              <pre>@article{Li2020Efficient,
    Author = {Zhuohan Li and Eric Wallace and Sheng Shen and Kevin Lin and Kurt Keutzer and Dan Klein and Joseph E. Gonzalez},
    Journal = {arXiv preprint arXiv:2002.11794},
    Year = {2020},
    Title = {Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers}}
               </pre>
               </div>
        </li>

        <li>        
                <a href="https://arxiv.org/abs/1908.07125" target="_blank" style="color:black;font-size:1.0em">Universal Adversarial Triggers for Attacking and Analyzing NLP</a><br>
                Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh<br>
                <i>EMNLP 2019</i><br>
                We create sequences of tokens that cause a model to produce a specific prediction when concatenated to <i>any</i> input from a dataset. Triggers cause egregious errors for text classification, reading comprehension (e.g., models predict "to kill american people" for 72% of "why" questions), and text generation (e.g., cause GPT-2 to generate racism).
          <br> <a href="https://vimeo.com/396789889" target="_blank">Video</a> | <a href="http://ericswallace.com/triggers" target="_blank">Blog</a> | <a href="https://twitter.com/Eric_Wallace_/status/1168907518623571974" target="_blank">Twitter</a> | <a href="https://www.wired.com/story/technique-uses-ai-fool-other-ais/" target="_blank">Wired</a> | <a href="https://arxiv.org/abs/1908.07125" target="_blank">Paper</a> | <a href="https://github.com/Eric-Wallace/universal-triggers" target="_blank">Code</a> | <a href="slides_and_posters/Universal_Adversarial_Triggers.pdf" target="_blank">Slides</a> | <a href="javascript:unhide('triggers19');">Citation</a>
                <div id="triggers19" class="hidden">
                    <pre>@inproceedings{Wallace2019Triggers,
    Author = {Eric Wallace and Shi Feng and Nikhil Kandpal and Matt Gardner and Sameer Singh},
    Booktitle = {Empirical Methods in Natural Language Processing},                            
    Year = {2019},
    Title = {Universal Adversarial Triggers for Attacking and Analyzing {NLP}}}
                    </pre>
                </div>
            </li>
            <li>
                <a href="https://arxiv.org/abs/1909.07940" target="_blank" style="color:black;font-size:1.0em">Do NLP Models Know Numbers? Probing Numeracy in Embeddings</a><br>
                Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner.<br>
                <i>EMNLP 2019</i><br>
                We show that pre-trained word embeddings (e.g., BERT, word2vec, ELMo, GloVe) capture number magnitude and order, e.g., they know that "74" is smaller than "eighty-two". This makes it easy to perform some numerical reasoning tasks.
                 <br> <a href="https://twitter.com/Eric_Wallace_/status/1174360279624192000" target="_blank">Twitter</a> | <a href="https://arxiv.org/abs/1909.07940" target="_blank">Paper</a> | <a href="https://github.com/Eric-Wallace/numeracy" target="_blank">Code</a> | <a href="slides_and_posters/NumeracyPoster.pdf" target="_blank">Poster</a> | <a href="javascript:unhide('numeracy19');">Citation</a>
                <div id="numeracy19" class="hidden">
                    <pre>@inproceedings{Wallace2019Numeracy,
    Author = {Eric Wallace and Yizhong Wang and Sujian Li and Sameer Singh and Matt Gardner},
    Booktitle = {Empirical Methods in Natural Language Processing},                            
    Year = {2019},
    Title = {Do {NLP} Models Know Numbers? Probing Numeracy in Embeddings}}
                    </pre>
                </div>
            </li>

            <li>                
                <a href="https://arxiv.org/abs/1909.09251" target="_blank" style="color:black;font-size:1.0em">AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models</a><br>
                Eric Wallace, Jens Tuyls, Junlin Wang, Sanjay Subramanian, Matt Gardner, and Sameer Singh.<br>
          <i>Demo at EMNLP 2019</i> &nbsp;&nbsp;&nbsp; <b><i>Best Demo Award</i></b><br>
                An open-source toolkit built on top of AllenNLP that makes it easy to interpret NLP models.<br>
                <a href="https://allennlp.org/interpret" target="_blank">Landing Page</a> | <a href="https://twitter.com/Eric_Wallace_/status/1176886627852898309" target="_blank">Twitter</a> | <a href="https://demo.allennlp.org/reading-comprehension" target="_blank">Demo</a> | <a href="https://arxiv.org/abs/1909.09251" target="_blank">Paper</a> | <a href="slides_and_posters/InterpretPoster.pdf" target="_blank">Poster</a> | <a href="javascript:unhide('interpret19');">Citation</a> 
                <div id="interpret19" class="hidden">
                    <pre>@inproceedings{Wallace2019AllenNLP,
    Author = {Eric Wallace and Jens Tuyls and Junlin Wang and Sanjay Subramanian and Matt Gardner and Sameer Singh},
    Booktitle = {Empirical Methods in Natural Language Processing},                            
    Year = {2019},
    Title = {{AllenNLP Interpret}: A Framework for Explaining Predictions of {NLP} Models}}
                    </pre>
                </div>
            </li>

      <li>
                <a href="https://arxiv.org/abs/1905.05778" target="_blank" style="color:black;font-size:1.0em">Misleading Failures of Partial-input Baselines</a><br>
                Shi Feng, Eric Wallace, and Jordan Boyd-Graber.<br>
                <i>ACL 2019</i><br>
                A common technique for dataset analysis is partial-input baselines (e.g., question-only or hypothesis-only models). We show how the failure of these baselines can be misleading, and solve "hard" SNLI examples using cheap tricks.<br> <a href="https://arxiv.org/abs/1905.05778" target="_blank">Paper</a> | <a href="slides_and_posters/Misleading_Poster.pdf" target="_blank">Poster</a> | <a href="javascript:unhide('misleading19');">Citation</a>
                <div id="misleading19" class="hidden">
          <pre>@inproceedings{Feng2019Misleading,
    Author = {Shi Feng and Eric Wallace and Jordan Boyd-Graber},
    Booktitle = {Association for Computational Linguistics},                            
    Year = {2019},
    Title = {Misleading Failures of Partial-input Baselines}}
                  </pre>
                </div>
            </li>
      
            <li>
                <a href="http://arxiv.org/abs/1906.02900" target="_blank" style="color:black;font-size:1.0em">Compositional Questions Do Not Necessitate Multi-hop Reasoning</a><br>
                Sewon Min*, Eric Wallace*, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, and Luke Zettlemoyer<br>
                <i>ACL 2019</i><br>
    We argue that constructing multi-hop QA datasets is non-trivial, and that existing datasets are simpler than expected. For instance, single-hop models can solve most of HotpotQA due to weak distractor paragraphs.<br>
        <a href="https://arxiv.org/abs/1906.02900" target="_blank">Paper</a> | <a href="slides_and_posters/Compositional_Slides.pdf" target="_blank">Slides</a> | <a href="https://github.com/shmsw25/single-hop-rc" target="_blank">Code</a> | <a href="javascript:unhide('multihop19');">Citation</a>
                <div id="multihop19" class="hidden">
          <pre>@inproceedings{Min2019Multihop,
    Author = {Sewon Min and Eric Wallace and Sameer Singh and Matt Gardner and Hannaneh Hajishirzi and Luke Zettlemoyer},
    Booktitle = {Association for Computational Linguistics},                            
    Year = {2019},
    Title = {Compositional Questions Do Not Necessitate Multi-hop Reasoning}}
                  </pre>
                </div>
            </li>
            <li>
                <a href="https://arxiv.org/abs/1809.02701" target="_blank" style="color:black;font-size:1.0em">Trick Me If You Can: Human-in-the-loop Generation of Adversarial Examples for Question Answering</a><br>
                Eric Wallace, Pedro Rodriguez, Shi Feng, Ikuya Yamada, and Jordan Boyd-Graber.<br>
                <i>TACL 2019</i><br>
                We propose a human-in-the-loop approach for generating adversarial examples in NLP. We display model intepretations and predictions in a user interface, which enables collaborative + interactive attacks on question answering systems .<br> <a href="https://arxiv.org/abs/1809.02701" target="_blank">Paper</a> | <a href="https://www.reddit.com/r/science/comments/cmzj8n/researchers_reveal_ai_weaknesses_by_developing/" target="_blank">Front page of Reddit</a> |
                <a href="http://trickme.qanta.org" target="_blank">Website</a> | <a href="https://github.com/Eric-Wallace/trickme-interface" target="_blank">Code</a> | <a href="slides_and_posters/TrickMe_Poster.pdf" target="_blank">Poster</a> | <a href="javascript:unhide('trick19');">Citation</a>
                <div id="trick19" class="hidden">
          <pre>@inproceedings{Wallace2019Trick,
    Author = {Eric Wallace and Pedro Rodriguez and Shi Feng and Ikuya Yamada and Jordan Boyd-Graber},
    Booktitle = {Transactions of the Association for Computational Linguistics},                            
    Year = {2019},
    Title = {Trick Me If You Can: Human-in-the-loop Generation of Adversarial Examples for Question Answering}}
                  </pre>
                </div>
            </li>
                 
            <li>
                <a href="https://arxiv.org/abs/1902.00407" target="_blank" style="color:black;font-size:1.0em">Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation</a><br>
                Sahil Singla, Eric Wallace, Shi Feng, Soheil Feizi.<br>
                <i>ICML 2019</i><br>
    Existing saliency interpretation methods focus on first-order information, i.e., they inspect the gradient of a model. This work designs an efficient <i>second-order</i> interpretation that can capture the loss function's curvature, i.e., the Hessian.
                <a href="https://arxiv.org/abs/1902.00407" target="_blank">Paper</a> | <a href="https://github.com/singlasahil14/CASO" target="_blank">Code</a> | <a href="javascript:unhide('caso19');">Citation</a>                
                <div id="caso19" class="hidden">
                    <pre>@inproceedings{Singla2019Caso,
    Author = {Sahil Singla and Eric Wallace and Shi Feng and Soheil Feizi},
    Booktitle = {International Conference on Machine Learning},                            
    Year = {2019},
    Title = {Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation}}
                  </pre>
                </div>
           </li>     
     <li>
                <a href="https://arxiv.org/abs/1804.07781" target="_blank" style="color:black;font-size:1.0em">Pathologies of Neural Models Make Interpretations Difficult</a><br>
                Shi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer, Pedro Rodriguez, Jordan Boyd-Graber.<br>
                <i>EMNLP 2018</i><br>
                Saliency maps are a popular interpretation technique. We show that certain pathological behavior present in neural models (namely prediction overconfidence) can negatively impact these interpretations.<br> <a href="https://vimeo.com/306158589" target="_blank">Video</a> | <a href="https://arxiv.org/abs/1804.07781" target="_blank">Paper</a> | 
                <a href="slides_and_posters/pathologies_slides.pdf" target="_blank">Slides</a> | <a href="https://soundcloud.com/nlp-highlights/87-pathologies-of-neural-models-make-interpretation-difficult-with-shi-feng" target="_blank">Podcast 1</a> <a href="https://twimlai.com/twiml-talk-229-pathologies-of-neural-models-and-interpretability-with-alvin-grissom-ii/" target="_blank">2</a> | <a href="https://github.com/allenai/allennlp/blob/master/allennlp/interpret/attackers/input_reduction.py" target="_blank">Code</a> | <a href="javascript:unhide('pathological18');">Citation</a>                
                <div id="pathological18" class="hidden">
                    <pre>@inproceedings{Feng2018Pathological,
    Author = {Shi Feng and Eric Wallace and Alvin Grissom II and Mohit Iyyer and Pedro Rodriguez and Jordan Boyd-Graber},
    Booktitle = {Empirical Methods in Natural Language Processing},                            
    Year = {2018},
    Title = {Pathologies of Neural Models Make Interpretations Difficult}}
                  </pre>
                </div>
            </li>
        </ul>  
        <br>
    </p>

    <p>Some recent presentations of my work!<br>
    <div class="box"><iframe src="https://player.vimeo.com/video/396789889?byline=0&portrait=0" width="100%" frameborder="0" allow="autoplay; fullscreen" allowfullscreen align="left"></iframe></div>    
    <div class="box"><iframe src="https://player.vimeo.com/video/306158589?byline=0&portrait=0" width="100%" frameborder="0" allow="autoplay; fullscreen" allowfullscreen align="right"></iframe>
    </p> 
  
  <p><a href="https://people.cs.umass.edu/~miyyer/">Mohit makes gdreat websites</a></p> 

</div>
</div>

</body>
</html>
