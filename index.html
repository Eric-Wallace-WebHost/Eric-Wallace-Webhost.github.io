<!DOCTYPE html>

<html>
<head>
	<meta charset="utf-8" />
	<title>Eric Wallace &mdash; Home</title>
	<link rel="stylesheet" href="master.css" />
	<link href="https://fonts.googleapis.com/css?family=Roboto+Slab:300,400,700" rel="stylesheet">

	<script>
	    function unhide(divID) {
	        var item = document.getElementById(divID);
	        if (item) {
	            item.className=(item.className=='hidden')?'unhidden':'hidden';
	        }
	}
	</script>

	<script type="text/javascript">
		var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
		document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
	</script>	
	
	
    <link rel="icon" href="http://nlp.cs.berkeley.edu/logo.png" />
    <meta name="author" content="Eric Wallace">
    <meta name="keywords" content="Eric Wallace Berkeley AI2">
    <meta name="robots" content="index,follow">
    <meta name="description" content="Homepage of Eric Wallace Berkeley">
	
</head>

<body>
<div class="wrapper">

	<div class="posts-wrapper">
		<div class="post">
			<img src='crop.gif' style="float:right; width:250px;"/>

			<h1>Eric Wallace</h1>

			<h2>ericwallace@berkeley.edu // <a style="font-size: 0.95em; font-weight:700" href="https://scholar.google.com/citations?user=SgST3LkAAAAJ" target="_blank"> Scholar </a> // <a style="font-size: 0.95em; font-weight:700" href="https://www.github.com/Eric-Wallace" target="_blank">GitHub</a> // <a style="font-size: 0.95em; font-weight:700" href="https://www.twitter.com/Eric_Wallace_" target="_blank">Twitter</a> // <a style="font-size: 0.95em; font-weight:700" href="CV.pdf" target="_blank">CV</a></h2>
			<br>
			<br>

            <p>I am a first-year PhD student at UC Berkeley advised by <a href="https://people.eecs.berkeley.edu/~klein/" target="_blank">Dan Klein</a> and <a href="https://people.eecs.berkeley.edu/~dawnsong/" target="_blank">Dawn Song</a>. I work on Machine Learning and Natural Language Processing as part of <a href="http://nlp.cs.berkeley.edu/" target="_blank">Berkeley NLP</a>, the <a href="https://rise.cs.berkeley.edu/" target="_blank">RISE Lab</a>, and Berkeley AI Research <a href="https://bair.berkeley.edu" target="_blank">(BAIR)</a>. 
            <br> <br>

            Before this, I did my undergrad at the University of Maryland, where I worked with <a href="http://www.umiacs.umd.edu/~jbg/" target="_blank">Jordan Boyd-Graber</a>. I spent most of 2019 working at the <a href="https://allenai.org/" target="_blank">Allen Institute for AI</a> with <a href="https://allenai.org/team/mattg/" target="_blank">Matt Gardner</a> and <a href="http://sameersingh.org/" target="_blank">Sameer Singh</a>.
            <br><br>

            <h3 style="margin-bottom:0.75em;">Research</h3>
	  	            
            <p><b>Interpretability</b> We look to open up the black box of machine learning by interpreting model predictions. We have analyzed the <a href="https://arxiv.org/abs/1804.07781" target="_blank" style="font-size: 0.95em">limitations</a> of existing interpretation methods and designed new techniques for generating <a href="https://arxiv.org/abs/1902.00407" target="_blank" style="font-size: 0.95em">saliency</a> <a href="https://arxiv.org/abs/1809.02847" target="_blank" style="font-size: 0.95em">maps</a>. We facilitate research and adoption of interpretation methods through an <a href="https://arxiv.org/abs/1909.09251" target="_blank">open-source interpretation toolkit</a>. Our current research probes the <a href="https://arxiv.org/abs/1909.07940" target="_blank" style="font-size: 0.95em">naturally emergent knowledge</a> of pre-trained NLP models and studies when interpretations are useful in practice.</p>

            <p><b>Robustness</b> We study how an adversary or a distributional shift can affect model behavior. We have created an adversarial attack called <a href="https://arxiv.org/abs/1908.07125" target="_blank" style="font-size: 0.95em">Universal Adversarial Triggers</a> that exposes egregious failures of state-of-the-art NLP systems. We have also used adversarial attacks to reveal counterintuitive model behavior on <a href="https://arxiv.org/abs/1804.07781" target="_blank" style="font-size: 0.95em">reduced</a> and  <a href="https://arxiv.org/abs/1809.02701" target="_blank" style="font-size: 0.95em">human-modified</a> inputs. Our current research works to increase model robustness and studies new attack vectors such as model stealing and data poisoning.

            <p><b>Dataset Biases</b> We investigate how models can use subtle dataset patterns (<i>annotation artifacts</i>) to achieve high test accuracy without truly understanding a task. We have analyzed <a href="https://arxiv.org/abs/1905.05778" target="_blank" style="font-size: 0.95em">techniques</a> for discovering these artifacts and applied them to identify issues in existing <a href="https://arxiv.org/abs/1906.02900" target="_blank" style="font-size: 0.95em">datasets</a>. We also study how overreliance on dataset biases can cause downstream failures such as susceptibility to <a href="https://arxiv.org/abs/1908.07125" target="_blank" style="font-size: 0.95em">adversarial attacks</a>. Our current research creates new ways to more comprehensively evaluate models.
		</p>
	        <br>
	    </div>
	</div>

	<div class="posts-wrapper" style="clear:both">
		<h3 style="margin-bottom:0.75em;">publications</h3>
		</i>
		<p>
		<ul class="pubs">
                
			
<!-- 			
			<li>        
    <a href="https://arxiv.org/abs/1908.07125" target="_blank" style="color:black;font-size:1.0em">Stealing and Attacking Production Machine Translation Systems</a><br>
    Eric Wallace, Mitchell Stern, Dawn Song, and Dan Klein.<br>
    <i>ACL 2020</i><br>
    TODO
    <br> <a href="https://arxiv.org/abs/1908.07125" target="_blank">Paper</a> | <a href="https://arxiv.org/abs/1908.07125" target="_blank">Code</a> | <a href="slides_and_posters/Universal_Adversarial_Triggers.pdf" target="_blank">Slides</a> | <a href="javascript:unhide('stealing20');">Citation</a>                
  <div id="stealing20" class="hidden">
  <pre>@inproceedings{Wallace2020Stealing,
    Author = {Eric Wallace and Mitchell Stern and Dawn Song and Dan Klein},
    Booktitle = {Association for Computational Linguistics},                            
    Year = {2020},
    Title = {Stealing and Attacking Production Machine Translation Systems}
   </pre>
   </div>
</li>

            
<li>        
    <a href="https://arxiv.org/abs/1908.07125" target="_blank" style="color:black;font-size:1.0em">Evaluating NLP Models via Contrast Sets</a><br>
    Matt Gardner, Yoav Artzi, ...., Eric Wallace, Ally Zhang, and Ben Zhou.<br>
    <i>ACL 2020</i><br>
    TODO
    <br> <a href="https://arxiv.org/abs/1908.07125" target="_blank">Dataset</a> | <a href="https://arxiv.org/abs/1908.07125" target="_blank">Paper</a> | <a href="slides_and_posters/Universal_Adversarial_Triggers.pdf" target="_blank">Poster</a> | <a href="javascript:unhide('contrast20');">Citation</a>                
  <div id="contrast20" class="hidden">
  <pre>@inproceedings{Gardner2020Contrast,
    Author = {Matt Gardner and Yoav Artzi and Victoria Basmov and Jonathan Berant and Ben Bogin and Sihao Chen and Pradeep Dasigi and Dheeru Dua and Yanai Elazar and Ananth Gottumukkala and Nitish Gupta and Hannaneh Hajishirzi and Gabriel Ilharco and Divyansh Kaushik and Daniel Khashabi and Kevin Lin and Zachary C. Lipton and Jiangming Liu and Nelson F. Liu and Phoebe Mulcaire and Qiang Ning and Sameer Singh and Noah A. Smith and Sanjay Subramanian and Reut Tsarfaty and Eric Wallace and Ally Zhang and Ben Zhou},
    Booktitle = {Association for Computational Linguistics},                            
    Year = {2020},
    Title = {Evaluating NLP Models via Contrast Sets}
   </pre>
   </div>
</li>

 
<li>        
    <a href="https://arxiv.org/abs/1908.07125" target="_blank" style="color:black;font-size:1.0em">BERT Is Moderately Robust to Distributional Shift</a><br>
    Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song<br>
    <i>ACL 2020</i><br>
    TODO
    <br> <a href="https://arxiv.org/abs/1908.07125" target="_blank">Paper</a> | <a href="slides_and_posters/Universal_Adversarial_Triggers.pdf" target="_blank">Poster</a> | <a href="javascript:unhide('robust20');">Citation</a>                
  <div id="robust20" class="hidden">
  <pre>@inproceedings{Hendrycks2020Bert,
    Author = {Dan Hendrycks and Xiaoyuan Liu and Eric Wallace and Adam Dziedzic Rishabh Krishnan and Dawn Song},
    Booktitle = {Association for Computational Linguistics},                            
    Year = {2020},
    Title = {{BERT} Is Moderately Robust to Distributional Shift}}
   </pre>
   </div>
</li>
            
			 -->
		<li>        
            <a href="https://arxiv.org/abs/2002.11794" target="_blank" style="color:black;font-size:1.0em">
            Train Big, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers</a><br>
            Zhuohan Li*, Eric Wallace*, Sheng Shen*, Kevin Lin*, Kurt Keutzer, Dan Klein, and Joseph E. Gonzalez<br>
            <i>arXiv Preprint</i><br>
            We show that <i>increasing</i> model size actually speeds up training and inference for Transformer models. The key idea is that you should use a very big model, but, do very few epochs and apply heavy compression.
            <br> <a href="https://bair.berkeley.edu/blog/2020/03/05/compress/" target="_blank">Blog</a> | <a href="https://arxiv.org/abs/2002.11794" target="_blank">Paper</a> | <a href="javascript:unhide('efficient20');">Citation</a>                
              <div id="efficient20" class="hidden">
              <pre>@article{Li2020Efficient,
    Author = {Zhuohan Li and Eric Wallace and Sheng Shen and Kevin Lin and Kurt Keutzer and Dan Klein and Joseph E. Gonzalez},
    Journal = {arXiv preprint arXiv:2002.11794},
    Year = {2020},
    Title = {Train Big, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers}}
               </pre>
               </div>
        </li>

        <li>        
                <a href="https://arxiv.org/abs/1908.07125" target="_blank" style="color:black;font-size:1.0em">Universal Adversarial Triggers for Attacking and Analyzing NLP</a><br>
                Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh<br>
                <i>EMNLP 2019</i><br>
                We create sequences of tokens that cause a model to produce a specific prediction when concatenated to <i>any</i> input from a dataset. Triggers cause egregious errors for text classification, reading comprehension (e.g., models predict "to kill american people" for 72% of "why" questions), and text generation (e.g., cause GPT-2 to generate racism).
                <br> <a href="http://ericswallace.com/triggers" target="_blank">Blog</a> | <a href="https://arxiv.org/abs/1908.07125" target="_blank">Paper</a> | <a href="https://github.com/Eric-Wallace/universal-triggers" target="_blank">Code</a> | <a href="slides_and_posters/Universal_Adversarial_Triggers.pdf" target="_blank">Slides</a> | <a href="javascript:unhide('triggers19');">Citation</a>
                <div id="triggers19" class="hidden">
                    <pre>@inproceedings{Wallace2019Triggers,
    Author = {Eric Wallace and Shi Feng and Nikhil Kandpal and Matt Gardner and Sameer Singh},
    Booktitle = {Empirical Methods in Natural Language Processing},                            
    Year = {2019},
    Title = {Universal Adversarial Triggers for Attacking and Analyzing {NLP}}}
                    </pre>
                </div>
            </li>
            <li>
                <a href="https://arxiv.org/abs/1909.07940" target="_blank" style="color:black;font-size:1.0em">Do NLP Models Know Numbers? Probing Numeracy in Embeddings</a><br>
                Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner.<br>
                <i>EMNLP 2019</i><br>
                We show that pre-trained word embeddings (e.g., BERT, word2vec, ELMo, GloVe) capture number magnitude and order, e.g., they know that "74" is smaller than "eighty-two". This makes it easy to perform some numerical reasoning tasks.
                 <br> <a href="https://arxiv.org/abs/1909.07940" target="_blank">Paper</a> | <a href="slides_and_posters/NumeracyPoster.pdf" target="_blank">Poster</a> | <a href="javascript:unhide('numeracy19');">Citation</a>
                <div id="numeracy19" class="hidden">
                    <pre>@inproceedings{Wallace2019Numeracy,
    Author = {Eric Wallace and Yizhong Wang and Sujian Li and Sameer Singh and Matt Gardner},
    Booktitle = {Empirical Methods in Natural Language Processing},                            
    Year = {2019},
    Title = {Do NLP Models Know Numbers? Probing Numeracy in Embeddings}}
                    </pre>
                </div>
            </li>

            <li>        		    
                <a href="https://arxiv.org/abs/1909.09251" target="_blank" style="color:black;font-size:1.0em">AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models</a><br>
                Eric Wallace, Jens Tuyls, Junlin Wang, Sanjay Subramanian, Matt Gardner, and Sameer Singh.<br>
	        <i>Demo at EMNLP 2019</i> &nbsp;&nbsp;&nbsp; <b><i>Best Demo Award</i></b><br>
                An open-source toolkit built on top of AllenNLP that makes it easy to interpret NLP models.<br> 
                <a href="https://allennlp.org/interpret" target="_blank">Landing Page</a> | <a href="https://demo.allennlp.org/reading-comprehension" target="_blank">Demo</a> | <a href="https://arxiv.org/abs/1909.09251" target="_blank">Paper</a> | <a href="slides_and_posters/InterpretPoster.pdf" target="_blank">Poster</a> | <a href="javascript:unhide('interpret19');">Citation</a> 
                <div id="interpret19" class="hidden">
                    <pre>@inproceedings{Wallace2019AllenNLP,
    Author = {Eric Wallace and Jens Tuyls and Junlin Wang and Sanjay Subramanian and Matt Gardner and Sameer Singh},
    Booktitle = {Empirical Methods in Natural Language Processing},                            
    Year = {2019},
    Title = {{AllenNLP Interpret}: A Framework for Explaining Predictions of {NLP} Models}}
                    </pre>
                </div>
            </li>
						
<!--             <li>
                <a href="https://distill.pub/2019/advex-bugs-discussion/response-6/" target="_blank" style="color:black;font-size:1.0em">Learning from Incorrectly Labeled Data</a><br>
                Eric Wallace<br>
                <i>Distill Pub</i><br>
                We take a completely mislabeled training set (without modifying the inputs) and use it to train a model that <i>generalizes</i> to the original test set. We attribute this and other recent results to indirect model distillation.<br> <a href="https://distill.pub/2019/advex-bugs-discussion/response-6/" target="_blank">Blog Post</a> | <a href="javascript:unhide('bugs19');">Citation</a>
                <div id="bugs19" class="hidden">
					<pre>@article{Wallace2019bugs,
  author = {Wallace, Eric},
  title = {A Discussion of 'Adversarial Examples Are Not Bugs, They Are Features': Learning from Incorrectly Labeled Data},
  journal = {Distill},
  year = {2019},
  note = {https://distill.pub/2019/advex-bugs-discussion/response-6},
  doi = {10.23915/distill.00019.6}}
	                </pre>
                </div>
            </li> -->
			
			<li>
                <a href="https://arxiv.org/abs/1905.05778" target="_blank" style="color:black;font-size:1.0em">Misleading Failures of Partial-input Baselines</a><br>
                Shi Feng, Eric Wallace, and Jordan Boyd-Graber.<br>
                <i>ACL 2019</i><br>
                A common technique for dataset analysis is partial-input baselines (e.g., question-only or hypothesis-only models). We show how the failure of these baselines can be misleading, and solve "hard" SNLI examples using cheap tricks.<br> <a href="https://arxiv.org/abs/1905.05778" target="_blank">Paper</a> | <a href="slides_and_posters/Misleading_Poster.pdf" target="_blank">Poster</a> | <a href="javascript:unhide('misleading19');">Citation</a>
                <div id="misleading19" class="hidden">
					<pre>@inproceedings{Feng2019Misleading,
    Author = {Shi Feng and Eric Wallace and Jordan Boyd-Graber},
    Booktitle = {Association for Computational Linguistics},                            
    Year = {2019},
    Title = {Misleading Failures of Partial-input Baselines}}
	                </pre>
                </div>
            </li>
			
            <li>
                <a href="http://arxiv.org/abs/1906.02900" target="_blank" style="color:black;font-size:1.0em">Compositional Questions Do Not Necessitate Multi-hop Reasoning</a><br>
                Sewon Min*, Eric Wallace*, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, and Luke Zettlemoyer<br>
                <i>ACL 2019</i><br>
		We argue that constructing multi-hop QA datasets is non-trivial, and that existing datasets are simpler than expected. For instance, single-hop models can solve most of HotpotQA due to weak distractor paragraphs.<br>
        <a href="https://arxiv.org/abs/1906.02900" target="_blank">Paper</a> | <a href="slides_and_posters/Compositional_Slides.pdf" target="_blank">Slides</a> | <a href="https://github.com/shmsw25/single-hop-rc" target="_blank">Code</a> | <a href="javascript:unhide('multihop19');">Citation</a>
                <div id="multihop19" class="hidden">
					<pre>@inproceedings{Min2019Multihop,
    Author = {Sewon Min and Eric Wallace and Sameer Singh and Matt Gardner and Hannaneh Hajishirzi and Luke Zettlemoyer},
    Booktitle = {Association for Computational Linguistics},                            
    Year = {2019},
    Title = {Compositional Questions Do Not Necessitate Multi-hop Reasoning}}
	                </pre>
                </div>
            </li>
            <li>
                <a href="https://arxiv.org/abs/1809.02701" target="_blank" style="color:black;font-size:1.0em">Trick Me If You Can: Human-in-the-loop Generation of Adversarial Examples for Question Answering</a><br>
                Eric Wallace, Pedro Rodriguez, Shi Feng, Ikuya Yamada, and Jordan Boyd-Graber.<br>
                <i>TACL 2019</i><br>
                We propose a human-in-the-loop approach for generating adversarial examples in NLP. We display model intepretations and predictions in a user interface, which enables collaborative + interactive attacks on question answering systems .<br> <a href="https://arxiv.org/abs/1809.02701" target="_blank">Paper</a> | <a href="https://www.reddit.com/r/science/comments/cmzj8n/researchers_reveal_ai_weaknesses_by_developing/" target="_blank">Front page of Reddit</a> |
                <a href="http://trickme.qanta.org" target="_blank">Website</a> | <a href="https://github.com/Eric-Wallace/trickme-interface" target="_blank">Code</a> | <a href="slides_and_posters/TrickMe_Poster.pdf" target="_blank">Poster</a> | <a href="javascript:unhide('trick19');">Citation</a>
                <div id="trick19" class="hidden">
					<pre>@inproceedings{Wallace2019Trick,
    Author = {Eric Wallace and Pedro Rodriguez and Shi Feng and Ikuya Yamada and Jordan Boyd-Graber},
    Booktitle = {Transactions of the Association for Computational Linguistics},                            
    Year = {2019},
    Title = {Trick Me If You Can: Human-in-the-loop Generation of Adversarial Examples for Question Answering.}}
	                </pre>
                </div>
            </li>
                 
            <li>
                <a href="https://arxiv.org/abs/1902.00407" target="_blank" style="color:black;font-size:1.0em">Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation</a><br>
                Sahil Singla, Eric Wallace, Shi Feng, Soheil Feizi.<br>
                <i>ICML 2019</i><br>
		Existing saliency interpretation methods focus on first-order information, i.e., they inspect the gradient of a model. This work designs an efficient <i>second-order</i> interpretation that can capture the loss function's curvature, i.e., the Hessian.
                <a href="https://arxiv.org/abs/1902.00407" target="_blank">Paper</a> | <a href="https://github.com/singlasahil14/CASO" target="_blank">Code</a> | <a href="javascript:unhide('caso19');">Citation</a>                
                <div id="caso19" class="hidden">
                    <pre>@inproceedings{Singla2019Caso,
    Author = {Sahil Singla and Eric Wallace and Shi Feng and Soheil Feizi},
    Booktitle = {International Conference on Machine Learning},                            
    Year = {2019},
    Title = {Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation}}
	                </pre>
                </div>
           </li>		 
	   <li>
                <a href="https://arxiv.org/abs/1804.07781" target="_blank" style="color:black;font-size:1.0em">Pathologies of Neural Models Make Interpretations Difficult</a><br>
                Shi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer, Pedro Rodriguez, Jordan Boyd-Graber.<br>
                <i>EMNLP 2018</i><br>
                Saliency maps are a popular interpretation technique. We show that certain pathological behavior present in neural models (namely prediction overconfidence) can negatively impact these interpretations.<br> <a href="https://vimeo.com/306158589" target="_blank">Video</a> | <a href="https://arxiv.org/abs/1804.07781" target="_blank">Paper</a> | 
                <a href="slides_and_posters/pathologies_slides.pdf" target="_blank">Slides</a> | <a href="javascript:unhide('pathological18');">Citation</a>                
                <div id="pathological18" class="hidden">
                    <pre>@inproceedings{Feng2018Pathological,
    Author = {Shi Feng and Eric Wallace and Alvin Grissom II and Mohit Iyyer and Pedro Rodriguez and Jordan Boyd-Graber},
    Booktitle = {Empirical Methods in Natural Language Processing},                            
    Year = {2018},
    Title = {Pathologies of Neural Models Make Interpretations Difficult}}
	                </pre>
                </div>
            </li>
        

            <li>
                <a href="https://arxiv.org/abs/1809.02847" style="color:black;font-size:1.0em">Interpreting Neural Networks with Nearest Neighbors</a><br>
                Eric Wallace*, Shi Feng*, Jordan Boyd-Graber.<br>
                <i>EMNLP BlackboxNLP Workshop 2018</i><br>
                We replace the standard softmax classifier with a k-nearest neighbors classifier. This provides a different model uncertainty metric which we use to generate high precision saliency maps for text classification.<br> <a href="https://arxiv.org/abs/1809.02847" target="_blank">Paper</a> | <a href="https://github.com/Eric-Wallace/deep-knn" target="_blank">Code</a> | <a href="https://zerobatchsize.net/2018/09/11/dknn.html">Blog</a> | <a href="javascript:unhide('knn18');">Citation</a>
                <div id="knn18" class="hidden">
                    <pre>@inproceedings{Wallace2018Neighbors,
    Author = {Eric Wallace and Shi Feng and Jordan Boyd-Graber},
    Booktitle = {EMNLP 2018 Workshop on Analyzing and Interpreting Neural Networks for {NLP}},                            
    Year = {2018},
    Title = {Interpreting Neural Networks with Nearest Neighbors}}
                 </pre>
                </div>
            </li>
        </ul>	 
        <br>
		</p>

		<p><a href="https://people.cs.umass.edu/~miyyer/">Mohit makes great websites</a></p> 

</div>
</div>

</body>
</html>
