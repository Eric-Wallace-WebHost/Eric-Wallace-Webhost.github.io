<!DOCTYPE html>

<html>
<head>
	<meta name="yandex-verification" content="a2b0dfa3a10b5338" />
	<meta charset="utf-8" />
	<title>Eric Wallace&mdash; Home</title>
	<link rel="stylesheet" href="master.css" />

	<link href="https://fonts.googleapis.com/css?family=Roboto+Slab:300,400,700" rel="stylesheet">

	<script>
	    function unhide(divID) {
	        var item = document.getElementById(divID);
	        if (item) {
	            item.className=(item.className=='hidden')?'unhidden':'hidden';
	        }
	}
	</script>

	<script type="text/javascript">
		var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
		document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
	</script>	
</head>

<body>
<div class="wrapper">

	<div class="posts-wrapper">
		<div class="post">
			<img src='crop.gif' style="float:right; width:250px;"/>

			<h1>Eric Wallace</h1>

			<h2>ericwallace@berkeley.edu // <a href="https://scholar.google.com/citations?user=SgST3LkAAAAJ" target="_blank"> Scholar </a> // <a href="https://www.github.com/Eric-Wallace" target="_blank">GitHub</a> // <a href="https://www.twitter.com/Eric_Wallace_" target="_blank">Twitter</a></h2>
			<br>
			<br>

            <p>I am a first-year PhD student at UC Berkeley advised by <a href="https://people.eecs.berkeley.edu/~klein/" target="_blank">Dan Klein</a> and <a href="https://people.eecs.berkeley.edu/~dawnsong/" target="_blank">Dawn Song</a>. &nbsp; I work on Machine Learning, Deep Learning, and Natural Language Processing as part of <a href="http://nlp.cs.berkeley.edu/" target="_blank">Berkeley NLP</a> and <a href="https://bair.berkeley.edu" target="_blank">BAIR</a>.
            <br> <br>

	  		My current research focuses on interpreting, attacking, and understanding machine learning models. For example, understanding when and why neural models fail [<a href="https://arxiv.org/abs/1804.07781" target="_blank">1</a>], designing new interpretation methods [<a href="https://arxiv.org/abs/1909.07940" target="_blank">2</a>], and crafting adversarial attacks for NLP systems [<a href="https://arxiv.org/abs/1908.07125" target="_blank">3</a>].
	  		<br><br>

			I did my undergrad at the University of Maryland, where I worked with <a href="http://www.umiacs.umd.edu/~jbg/" target="_blank">Jordan Boyd-Graber</a>. I spent most of 2019 working at <a href="https://allenai.org/" target="_blank">AI2</a> with <a href="https://allenai.org/team/mattg/" target="_blank">Matt Gardner</a> and <a href="http://sameersingh.org/" target="_blank">Sameer Singh</a>.
	        </p>
	        <br>
	    </div>
	</div>

	<div class="posts-wrapper">
		<div class="post">
			<ul class="news">                        
            <li><strong>Aug 2019: </strong> Three papers accepted to EMNLP.
            <li><strong>Aug 2019: </strong> Began my PhD at UC Berkeley!                        
			<li><strong>May 2019: </strong> Two papers accepted to ACL.
			<li><strong>April 2019: </strong> Our paper <a href="https://arxiv.org/abs/1809.02701" target="_blank">Trick Me If You Can</a> was accepted to TACL 2019.
			<li><strong>April 2019: </strong> Our paper on a <a href="https://arxiv.org/abs/1902.00407" target="_blank">second-order interpretation method</a> was accepted to ICML 2019.
			<li><strong>Jan. 2019: </strong> Graduated from UMD and moved to the Allen Institute for AI (AI2).
			<li><strong>Nov. 2018: </strong>Presented <a href="https://arxiv.org/abs/1809.02847" target="_blank">two</a> <a href="https://arxiv.org/abs/1804.07781" target="_blank">works</a> at EMNLP 2018 in Brussels, Belgium. <a href="https://vimeo.com/306158589" target="_blank">Video</a> here.</li>
			<li><strong>Sep. 2018: </strong>We're hosting a <a href="http://www.qanta.org" target="_blank"> competition</a> to develop Question Answering systems that can combat adversarial users </li>
			<li><strong>Aug. 2018: </strong></strong><a href="https://arxiv.org/abs/1809.02847" target="_blank">Paper</a> on a new method to interpret neural NLP models accepted at EMNLP Interpretability Workshop</li>
			<li><strong>Aug. 2018: </strong><a href="https://arxiv.org/abs/1804.07781" target="_blank">Paper</a> on the difficulties of interpreting neural models accepted at EMNLP</li>			
			<li><strong>May. 2018: </strong>Joining Lyft Self Driving as an intern this Summer in Palo Alto, CA</li></ul>

		</div>
	</div>
	<br><br>

    <a href="http://ericswallace.com/positive-movies.html"> <font color="white">Positive</font>
 </a>
    <a href="http://ericswallace.com/negative-movies.html"> <font color="white">Negative</font> </a>

	<div class="posts-wrapper" style="clear:both">
		<h3 style="margin-bottom:0.75em;">publications</h3>
		</i>
		<p>
		<ul class="pubs">
                    <li>        
                (New!) <a href="https://arxiv.org/abs/1908.07125" target="_blank">Universal Adversarial Triggers for Attacking and Analyzing NLP</a><br>
                Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh<br>
                <i>EMNLP 2019</i><br>
                We create sequences of tokens that cause a model to produce a specific prediction when concatenated to <i>any</i> input from a dataset. Triggers cause egregious errors for text classification, reading comprehension (e.g., models predict "to kill american people" for 72% of "why" questions), and text generation (e.g., cause GPT-2 to generate racism).
                <br> <a href="http://ericswallace.com/triggers" target="_blank">Blog</a> | <a href="https://arxiv.org/abs/1908.07125" target="_blank">Paper</a> | <a href="https://github.com/Eric-Wallace/universal-triggers" target="_blank">Code</a> | <a href="javascript:unhide('triggers19');">Citation</a>
                <div id="triggers19" class="hidden">
                    <pre>@inproceedings{Wallace2019Triggers,
    Author = {Eric Wallace and Shi Feng and Nikhil Kandpal and Matt Gardner and Sameer Singh},
    Booktitle = {Empirical Methods in Natural Language Processing},                            
    Year = {2019},
    Title = {Universal Adversarial Triggers for Attacking and Analyzing {NLP}}}
                    </pre>
                </div>
            </li>
            <li>
                (New!) <a href="https://arxiv.org/abs/1909.07940" target="_blank">Do NLP Models Know Numbers? Probing Numeracy in Embeddings</a><br>
                Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner.<br>
                <i>EMNLP 2019</i><br>
                We show that pre-trained word embeddings (e.g., BERT, word2vec, ELMo, GloVe) capture number magnitude and order, e.g., they know that "74" is smaller than "eighty-two". This makes it easy to perform some numerical reasoning tasks.
                 <br> <a href="https://arxiv.org/abs/1909.07940" target="_blank">Paper</a> | <a href="javascript:unhide('numeracy19');">Citation</a>
                <div id="numeracy19" class="hidden">
                    <pre>@inproceedings{Wallace2019Numeracy,
    Author = {Eric Wallace and Yizhong Wang and Sujian Li and Sameer Singh and Matt Gardner},
    Booktitle = {Empirical Methods in Natural Language Processing},                            
    Year = {2019},
    Title = {Do NLP Models Know Numbers? Probing Numeracy in Embeddings}}
                    </pre>
                </div>
            </li>

            <li>        		    
                (New!) <a href="link" target="_blank">AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models</a><br>
                Eric Wallace, Jens Tuyls, Junlin Wang, Sanjay Subramanian, Matt Gardner, and Sameer Singh.<br>
	        <i>Demo at EMNLP 2019</i> &nbsp;&nbsp;&nbsp; <b><i>Best Demo Award</i></b><br>
                An open-source toolkit built on top of AllenNLP that makes it easy to interpret NLP models.<br> 
                <a href="https://allennlp.org/interpret" target="_blank">Landing Page</a> | <a href="https://demo.allennlp.org/reading-comprehension" target="_blank">Demo</a> | <a href="link" target="_blank">Paper</a> | <a href="javascript:unhide('interpret19');">Citation</a> 
                <div id="interpret19" class="hidden">
                    <pre>@inproceedings{Wallace2019AllenNLP,
    Author = {Eric Wallace and Jens Tuyls and Junlin Wang and Sanjay Subramanian and Matt Gardner and Sameer Singh},
    Booktitle = {Empirical Methods in Natural Language Processing},                            
    Year = {2019},
    Title = {{AllenNLP Interpret}: A Framework for Explaining Predictions of {NLP} Models}}
                    </pre>
                </div>
            </li>
						
            <li>
                (New!) <a href="https://arxiv.org/abs/1905.05778" target="_blank">Learning from Incorrectly Labeled Data</a><br>
                Eric Wallace<br>
                <i>Distill Pub</i><br>
                We take a completely mislabeled training set (without modifying the inputs) and use it to train a model that <i>generalizes</i> to the original test set. We attribute this and other recent results to indirect model distillation.<br> <a href="https://distill.pub/2019/advex-bugs-discussion/response-6/" target="_blank">Blog Post</a> | <a href="javascript:unhide('bugs19');">Citation</a>
                <div id="bugs19" class="hidden">
					<pre>@article{Wallace2019bugs,
  author = {Wallace, Eric},
  title = {A Discussion of 'Adversarial Examples Are Not Bugs, They Are Features': Learning from Incorrectly Labeled Data},
  journal = {Distill},
  year = {2019},
  note = {https://distill.pub/2019/advex-bugs-discussion/response-6},
  doi = {10.23915/distill.00019.6}}
	                </pre>
                </div>
            </li>
			
			<li>
                (New!) <a href="https://arxiv.org/abs/1905.05778" target="_blank">Misleading Failures of Partial-input Baselines</a><br>
                Shi Feng, Eric Wallace, and Jordan Boyd-Graber.<br>
                <i>ACL 2019</i><br>
                A common technique for dataset analysis is partial-input baselines (e.g., question-only or hypothesis-only models). We show how the failure of these baselines can be misleading, and solve "hard" SNLI examples using cheap tricks.<br> <a href="https://arxiv.org/abs/1905.05778" target="_blank">Paper</a> | <a href="Misleading_Poster.pdf" target="_blank">Poster</a> | <a href="javascript:unhide('misleading19');">Citation</a>
                <div id="misleading19" class="hidden">
					<pre>@inproceedings{Feng2019Misleading,
    Author = {Shi Feng and Eric Wallace and Jordan Boyd-Graber},
    Booktitle = {Association for Computational Linguistics},                            
    Year = {2019},
    Title = {Misleading Failures of Partial-input Baselines}}
	                </pre>
                </div>
            </li>
			
            <li>
                (New!) <a href="http://arxiv.org/abs/1906.02900" target="_blank">Compositional Questions Do Not Necessitate Multi-hop Reasoning</a><br>
                Sewon Min*, Eric Wallace*, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, and Luke Zettlemoyer<br>
                <i>ACL 2019</i><br>
		We argue that constructing multi-hop QA datasets is non-trivial, and that existing datasets are simpler than expected. For instance, single-hop models can solve most of HotpotQA due to weak distractor paragraphs.<br>
        <a href="https://arxiv.org/abs/1906.02900" target="_blank">Paper</a> | <a href="Compositional_Slides.pdf" target="_blank">Slides</a> | <a href="https://github.com/shmsw25/single-hop-rc" target="_blank">Code</a> | <a href="javascript:unhide('multihop19');">Citation</a>
                <div id="multihop19" class="hidden">
					<pre>@inproceedings{Min2019Multihop,
    Author = {Sewon Min and Eric Wallace and Sameer Singh and Matt Gardner and Hannaneh Hajishirzi and Luke Zettlemoyer},
    Booktitle = {Association for Computational Linguistics},                            
    Year = {2019},
    Title = {Compositional Questions Do Not Necessitate Multi-hop Reasoning}}
	                </pre>
                </div>
            </li>
            <li>
                (New!) <a href="https://arxiv.org/abs/1809.02701" target="_blank">Trick Me If You Can: Human-in-the-loop Generation of Adversarial Examples for Question Answering</a><br>
                Eric Wallace, Pedro Rodriguez, Shi Feng, Ikuya Yamada, and Jordan Boyd-Graber.<br>
                <i>TACL 2019</i><br>
                We propose a human-in-the-loop approach for generating adversarial examples in NLP. We display model intepretations and predictions in a user interface, which enables collaborative + interactive attacks on question answering systems .<br> <a href="https://arxiv.org/abs/1809.02701" target="_blank">Paper</a> | <a href="https://www.reddit.com/r/science/comments/cmzj8n/researchers_reveal_ai_weaknesses_by_developing/" target="_blank">Front page of Reddit</a> |
                <a href="http://trickme.qanta.org" target="_blank">Website</a> | <a href="https://github.com/Eric-Wallace/trickme-interface" target="_blank">Code</a> | <a href="TrickMe_Poster.pdf" target="_blank">Poster</a> | <a href="javascript:unhide('trick19');">Citation</a>
                <div id="trick19" class="hidden">
					<pre>@inproceedings{Wallace2019Trick,
    Author = {Eric Wallace and Pedro Rodriguez and Shi Feng and Ikuya Yamada and Jordan Boyd-Graber},
    Booktitle = {Transactions of the Association for Computational Linguistics},                            
    Year = {2019},
    Title = {Trick Me If You Can: Human-in-the-loop Generation of Adversarial Examples for Question Answering.}}
	                </pre>
                </div>
            </li>
                 
            <li>
                <a href="https://arxiv.org/abs/1902.00407" target="_blank">Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation</a><br>
                Sahil Singla, Eric Wallace, Shi Feng, Soheil Feizi.<br>
                <i>ICML 2019</i><br>
		Existing saliency interpretation methods focus on first-order information, i.e., they inspect the gradient of a model. This work designs an efficient <i>second-order</i> interpretation that can capture the loss function's curvature, i.e., the Hessian.
                <a href="https://arxiv.org/abs/1902.00407" target="_blank">Paper</a> | <a href="https://github.com/singlasahil14/CASO" target="_blank">Code</a> | <a href="javascript:unhide('caso19');">Citation</a>                
                <div id="caso19" class="hidden">
                    <pre>@inproceedings{Singla2019Caso,
    Author = {Sahil Singla and Eric Wallace and Shi Feng and Soheil Feizi},
    Booktitle = {International Conference on Machine Learning},                            
    Year = {2019},
    Title = {Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation}}
	                </pre>
                </div>
           </li>		 
	   <li>
                <a href="https://arxiv.org/abs/1804.07781" target="_blank">Pathologies of Neural Models Make Interpretations Difficult</a><br>
                Shi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer, Pedro Rodriguez, Jordan Boyd-Graber.<br>
                <i>EMNLP 2018</i><br>
                Saliency maps are a popular interpretation technique. We show that certain pathological behavior present in neural models (namely prediction overconfidence) can negatively impact these interpretations.<br> <a href="https://vimeo.com/306158589" target="_blank"Video</a> | <a href="https://arxiv.org/abs/1804.07781" target="_blank">Paper</a> | 
                <a href="https://github.com/Pinafore/publications/raw/master/2018_emnlp_pathologies/slides/slides.pdf" target="_blank">Slides</a> | <a href="javascript:unhide('pathological18');">Citation</a>                
                <div id="pathological18" class="hidden">
                    <pre>@inproceedings{Feng2018Pathological,
    Author = {Shi Feng and Eric Wallace and Alvin Grissom II and Mohit Iyyer and Pedro Rodriguez and Jordan Boyd-Graber},
    Booktitle = {Empirical Methods in Natural Language Processing},                            
    Year = {2018},
    Title = {Pathologies of Neural Models Make Interpretations Difficult}}
	                </pre>
                </div>
            </li>
        

            <li>
                <a href="https://arxiv.org/abs/1809.02847">Interpreting Neural Networks with Nearest Neighbors</a><br>
                Eric Wallace*, Shi Feng*, Jordan Boyd-Graber.<br>
                <i>EMNLP BlackboxNLP Workshop 2018</i><br>
                We replace the standard softmax classifier with a k-nearest neighbors classifier. This provides a different model uncertainty metric which we use to generate high precision saliency maps for text classification.<br> <a href="https://arxiv.org/abs/1809.02847" target="_blank">Paper</a> | <a href="https://github.com/Eric-Wallace/deep-knn" target="_blank">Code</a> | <a href="https://zerobatchsize.net/2018/09/11/dknn.html">Blog</a> | <a href="javascript:unhide('knn18');">Citation</a>
                <div id="knn18" class="hidden">
                    <pre>@inproceedings{Wallace2018Neighbors,
    Author = {Eric Wallace and Shi Feng and Jordan Boyd-Graber},
    Booktitle = {EMNLP 2018 Workshop on Analyzing and Interpreting Neural Networks for {NLP}},                            
    Year = {2018},
    Title = {Interpreting Neural Networks with Nearest Neighbors}}
                 </pre>
                </div>
            </li>
        </ul>	 
        <br>
		</p>

		<p><a href="https://people.cs.umass.edu/~miyyer/">Mohit makes great websites</a></p> 

</div>
</div>

</body>
</html>
