<!DOCTYPE html>

<html>
<head>
  <meta charset="utf-8" />
  <title>Eric Wallace &mdash; Home</title>
  <link rel="stylesheet" href="master.css" />  
  <link href="https://fonts.googleapis.com/css?family=Roboto+Slab:300,400,700" rel="stylesheet">

  <script>
      function unhide(divID) {
          var item = document.getElementById(divID);
          if (item) {
              item.className=(item.className=='hidden')?'unhidden':'hidden';
          }
      }  
  </script>
  
    <link rel="icon" href="http://nlp.cs.berkeley.edu/logo.png" />
    <meta name="author" content="Eric Wallace">
    <meta name="keywords" content="Eric Wallace Berkeley AI2 NLP">
    <meta name="robots" content="index,follow">
    <meta name="description" content="Homepage of Eric Wallace Berkeley AI2 NLP">
  
</head>

<body>
<div class="wrapper">

  <div class="posts-wrapper">
    <div class="post">    
      <img src='full.jpg' style="float:right; width:275px"/>             

      <h1>Eric Wallace</h1>

      <h2>ericwallace@berkeley.edu | <a style="font-size: 0.95em; font-weight:700" href="https://www.twitter.com/Eric_Wallace_" target="_blank">Twitter</a> | <a style="font-size: 0.95em; font-weight:700" href="https://scholar.google.com/citations?user=SgST3LkAAAAJ" target="_blank"> Scholar </a> | <a style="font-size: 0.95em; font-weight:700" href="https://www.github.com/Eric-Wallace" target="_blank">GitHub</a> |
       <a style="font-size: 0.95em; font-weight:700" href="CV.pdf" target="_blank">CV</a></h2>
      <br>
      <br>

            <p>Hi! I am a second-year PhD student at UC Berkeley advised by <a href="https://people.eecs.berkeley.edu/~klein/" target="_blank">Dan Klein</a> and <a href="https://people.eecs.berkeley.edu/~dawnsong/" target="_blank">Dawn Song</a>. I work on Machine Learning and Natural Language Processing as part of <a href="http://nlp.cs.berkeley.edu/" target="_blank">Berkeley NLP</a>, the <a href="https://rise.cs.berkeley.edu/" target="_blank">RISE Lab</a>, and Berkeley AI Research <a href="https://bair.berkeley.edu" target="_blank">(BAIR)</a>. 
            <br> <br>

            Before this, I did my undergrad at the University of Maryland, where I worked with <a href="http://www.umiacs.umd.edu/~jbg/" target="_blank">Jordan Boyd-Graber</a>. I spent most of 2019 working at the <a href="https://allenai.org/" target="_blank">Allen Institute for AI</a> with <a href="https://allenai.org/team/mattg/" target="_blank">Matt Gardner</a> and <a href="http://sameersingh.org/" target="_blank">Sameer Singh</a>.
            <br><br>

            <h3 style="margin-bottom:0.75em;">Research Focus</h3>

            <p><b>Robustness</b> We study how an adversary or a <a href="https://arxiv.org/abs/2004.06100" target="_blank" style="font-size: 0.95em">distribution shift</a> can affect model behavior. We have an attack called <a href="https://arxiv.org/abs/1908.07125" target="_blank" style="font-size: 0.95em">Universal Adversarial Triggers</a> that exposes insightful failures of state-of-the-art NLP systems. We have revealed counterintuitive model behavior on <a href="https://arxiv.org/abs/1804.07781" target="_blank" style="font-size: 0.95em">reduced</a> and  <a href="https://arxiv.org/abs/1809.02701" target="_blank" style="font-size: 0.95em">human-modified</a> inputs. Our current research works to increase model robustness and studies new attack vectors such as <a href="https://arxiv.org/abs/2004.15015" target="_blank" style="font-size: 0.95em">model stealing</a> and <a href="" style="font-size: 0.95em">data poisoning</a>.</p>

            <p><b>Interpretability</b> We look to open up the black box of machine learning by interpreting model predictions. We facilitate research and adoption of interpretation methods with our <a href="https://arxiv.org/abs/1909.09251" target="_blank" style="font-size: 0.95em">open-source toolkit</a> (also see our <a href="" style="font-size: 0.95em">EMNLP tutorial</a>). We have analyzed the <a href="https://arxiv.org/abs/1804.07781" target="_blank" style="font-size: 0.95em">limitations</a> of interpretation methods and proposed new <a href="https://arxiv.org/abs/1902.00407" target="_blank" style="font-size: 0.95em">saliency map methods</a>.  Our current research <a href="https://arxiv.org/abs/1909.07940" target="_blank" style="font-size: 0.95em">probes</a> pretrained models and studies how <a href="" style="font-size: 0.95em">training examples affect test predictions</a>.</p>

            <p><b>Dataset Biases</b> We investigate how models can use spurious dataset patterns to achieve high accuracy without true understanding. We have analyzed <a href="https://arxiv.org/abs/1905.05778" target="_blank" style="font-size: 0.95em">techniques</a> for discovering such patterns and identified issues in existing <a href="https://arxiv.org/abs/1906.02900" target="_blank" style="font-size: 0.95em">annotation schemes</a>. Our current research looks to create better <a href="https://arxiv.org/abs/2004.02709" target="_blank" style="font-size: 0.95em">model evaluations</a>.
    </p>
          <br>
      </div>
  </div>

  <div class="posts-wrapper" style="clear:both">
    <h3 style="margin-bottom:0.75em;">publications</h3>
    </i>
    <p>

    <ul class="pubs">
    Hello whats up Hello whats up Hello whats up Hello whats up Hello whats up Hello whats up Hello whats up Hello whats up Hello whats up Hello whats up Hello whats up Hello whats up Hello whats up Hello whats up Hello whats up Hello whats up Hello whats up Hello whats up Hello whats up Hello whats up Hello whats up Hello whats up Hello whats up Hello whats up Hello whats up  
    <li> Hello my name is Eric Wallace. Hello my name is Eric Wallace. Hello my name is Eric Wallace. Hello my name is Eric Wallace. Hello my name is Eric Wallace. Hello my name is Eric Wallace. Hello my name is Eric Wallace. Hello my name is Eric Wallace. Hello my name is Eric Wallace. Hello my name is Eric Wallace. Hello my name is Eric Wallace. </li>
    <li> Hello my name is Eric Wallace. Hello my name is Eric Wallace. Hello my name is Eric Wallace. Hello my name is Eric Wallace. Hello my name is Eric Wallace. Hello my name is Eric Wallace. Hello my name is Eric Wallace. Hello my name is Eric Wallace. Hello my name is Eric Wallace. Hello my name is Eric Wallace. Hello my name is Eric Wallace. </li>  
    </ul>

    <ul class="pubs">
                
    <li>        
          <a href="https://arxiv.org/abs/2002.11794" target="_blank" style="color:black;font-size:1.0em">
          Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers</a><br>
          Zhuohan Li*, Eric Wallace*, Sheng Shen*, Kevin Lin*, Kurt Keutzer, Dan Klein, and Joseph E. Gonzalez<br>
          <i>ICML 2020</i><br>
          <div id="efficient20tldr" class="hidden"><b>TLDR:</b> We show that <i>increasing</i> model size actually speeds up training and inference for Transformer models. The key idea is to use a very large model, but, perform very few epochs and apply heavy compression.<br></div>
          <a href="javascript:unhide('efficient20tldr');">TLDR</a> | <a href="https://bair.berkeley.edu/blog/2020/03/05/compress/" target="_blank">Blog</a> | <a href="https://twitter.com/Eric_Wallace_/status/1235616760595791872" target="_blank">Twitter</a> | <a href="https://arxiv.org/abs/2002.11794" target="_blank">Paper</a> | <a href="slides_and_posters/train_large.pdf" target="_blank">Slides</a> | <a href="javascript:unhide('efficient20');">Citation</a>                
            <div id="efficient20" class="hidden">
            <pre>@inproceedings{Li2020Efficient,
  Author = {Zhuohan Li and Eric Wallace and Sheng Shen and Kevin Lin and Kurt Keutzer and Dan Klein and Joseph E. Gonzalez},
  Booktitle = {International Conference on Machine Learning},
  Year = {2020},
  Title = {Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers}}
             </pre>
             </div>
    </li>

    <li>        
    <a href="https://arxiv.org/abs/2004.06100" target="_blank" style="color:black;font-size:1.0em">Pretrained Transformers Improve Out-of-Distribution Robustness</a><br>
    Dan Hendrycks*, Xiaoyuan Liu*, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song<br>
    <i>ACL 2020</i><br>
    <div id="robust20tldr" class="hidden"><b>TLDR:</b> How does pretraining affect <i>out-of-distribution</i> robustness? We create an OOD benchmark and use it to show that pretraining substantially improves OOD accuracy and detection rates.<br></div>
    <a href="javascript:unhide('robust20tldr');">TLDR</a> | <a href="https://arxiv.org/abs/2004.06100" target="_blank">Paper</a> | <a href="https://twitter.com/Eric_Wallace_/status/1250507707674578944" target="_blank">Twitter</a> | <a href="https://github.com/camelop/NLP-Robustness" target="_blank">Code</a> | <a href="slides_and_posters/ood_robustness.pdf" target="_blank">Slides</a> | <a href="javascript:unhide('robust20');">Citation</a>
  <div id="robust20" class="hidden">
  <pre>@inproceedings{hendrycks2020pretrained,
    Author = {Dan Hendrycks and Xiaoyuan Liu and Eric Wallace and Adam Dziedzic and Rishabh Krishnan and Dawn Song},
    Booktitle = {Association for Computational Linguistics},                            
    Year = {2020},
    Title = {Pretrained Transformers Improve Out-of-Distribution Robustness}}
   </pre>
   </div>   
   </li>
        
    

        <li>        
                <a href="https://arxiv.org/abs/1908.07125" target="_blank" style="color:black;font-size:1.0em">Universal Adversarial Triggers for Attacking and Analyzing NLP</a><br>
                Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh<br>
                <i>EMNLP 2019</i><br>
                <div id="triggers19tldr" class="hidden"><b>TLDR:</b> We create phrases that cause a model to produce a specific prediction when concatenated to <i>any</i> input. Triggers reveal egregious and insightful errors for text classification, reading comprehension, and text generation.<br> </div>
          <a href="javascript:unhide('triggers19tldr');">TLDR</a> | <a href="https://vimeo.com/396789889" target="_blank">Video</a> | <a href="http://ericswallace.com/triggers" target="_blank">Blog</a> | <a href="https://twitter.com/Eric_Wallace_/status/1168907518623571974" target="_blank">Twitter</a> | <a href="https://arxiv.org/abs/1908.07125" target="_blank">Paper</a> | <a href="https://github.com/Eric-Wallace/universal-triggers" target="_blank">Code</a> | <a href="slides_and_posters/Universal_Adversarial_Triggers.pdf" target="_blank">Slides</a> | <a href="javascript:unhide('triggers19');">Citation</a>
                <div id="triggers19" class="hidden">
                    <pre>@inproceedings{Wallace2019Triggers,
    Author = {Eric Wallace and Shi Feng and Nikhil Kandpal and Matt Gardner and Sameer Singh},
    Booktitle = {Empirical Methods in Natural Language Processing},
    Year = {2019},
    Title = {Universal Adversarial Triggers for Attacking and Analyzing {NLP}}}
                    </pre>
                </div>
            </li>
            <li>
                <a href="https://arxiv.org/abs/1909.07940" target="_blank" style="color:black;font-size:1.0em">Do NLP Models Know Numbers? Probing Numeracy in Embeddings</a><br>
                Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner<br>
                <i>EMNLP 2019</i><br>
                <div id="numeracy19tldr" class="hidden"><b>TLDR:</b> We show that pre-trained word embeddings (e.g., BERT, word2vec, ELMo, GloVe) capture number magnitude and order, e.g., they know that "74" is smaller than "eighty-two". This facilitates basic numerical reasoning tasks. <br></div>
                 <a href="javascript:unhide('numeracy19tldr');">TLDR</a> | <a href="https://twitter.com/Eric_Wallace_/status/1174360279624192000" target="_blank">Twitter</a> | <a href="https://arxiv.org/abs/1909.07940" target="_blank">Paper</a> | <a href="https://github.com/Eric-Wallace/numeracy" target="_blank">Code</a> | <a href="slides_and_posters/NumeracyPoster.pdf" target="_blank">Poster</a> | <a href="javascript:unhide('numeracy19');">Citation</a>
                <div id="numeracy19" class="hidden">
                    <pre>@inproceedings{Wallace2019Numeracy,
    Author = {Eric Wallace and Yizhong Wang and Sujian Li and Sameer Singh and Matt Gardner},
    Booktitle = {Empirical Methods in Natural Language Processing},                            
    Year = {2019},
    Title = {Do {NLP} Models Know Numbers? Probing Numeracy in Embeddings}}
                    </pre>
                </div>
            </li>

            <li>                
                <a href="https://arxiv.org/abs/1909.09251" target="_blank" style="color:black;font-size:1.0em">AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models</a><br>
                Eric Wallace, Jens Tuyls, Junlin Wang, Sanjay Subramanian, Matt Gardner, and Sameer Singh<br>
          <i>Demo at EMNLP 2019</i> &nbsp;&nbsp;&nbsp; <b><i>Best Demo Award</i></b><br>
                <div id="interpret19tldr" class="hidden"><b>TLDR:</b> An open-source toolkit built on top of AllenNLP that makes it easy to interpret NLP models.<br> </div>
                <a href="javascript:unhide('interpret19tldr');">TLDR</a> | <a href="https://allennlp.org/interpret" target="_blank">Landing Page</a> | <a href="https://twitter.com/Eric_Wallace_/status/1176886627852898309" target="_blank">Twitter</a> | <a href="https://demo.allennlp.org/reading-comprehension" target="_blank">Demo</a> | <a href="https://arxiv.org/abs/1909.09251" target="_blank">Paper</a> | <a href="slides_and_posters/InterpretPoster.pdf" target="_blank">Poster</a> | <a href="javascript:unhide('interpret19');">Citation</a> 
                <div id="interpret19" class="hidden">
                    <pre>@inproceedings{Wallace2019AllenNLP,
    Author = {Eric Wallace and Jens Tuyls and Junlin Wang and Sanjay Subramanian and Matt Gardner and Sameer Singh},
    Booktitle = {Empirical Methods in Natural Language Processing},                            
    Year = {2019},
    Title = {{AllenNLP Interpret}: A Framework for Explaining Predictions of {NLP} Models}}
                    </pre>
                </div>
            </li>

      <li>
                <a href="https://arxiv.org/abs/1905.05778" target="_blank" style="color:black;font-size:1.0em">Misleading Failures of Partial-input Baselines</a><br>
                Shi Feng, Eric Wallace, and Jordan Boyd-Graber<br>
                <i>ACL 2019</i><br>
                <div id="misleading19tldr" class="hidden"><b>TLDR:</b> A common technique for analyzing datasets is partial-input baselines (e.g., question-only or hypothesis-only models). We show how these baselines can be misleading, and solve "hard" SNLI examples using cheap tricks.<br></div> 
                <a href="javascript:unhide('misleading19tldr');">TLDR</a> | <a href="https://arxiv.org/abs/1905.05778" target="_blank">Paper</a> | <a href="slides_and_posters/Misleading_Poster.pdf" target="_blank">Poster</a> | <a href="javascript:unhide('misleading19');">Citation</a>
                <div id="misleading19" class="hidden">
          <pre>@inproceedings{Feng2019Misleading,
    Author = {Shi Feng and Eric Wallace and Jordan Boyd-Graber},
    Booktitle = {Association for Computational Linguistics},                            
    Year = {2019},
    Title = {Misleading Failures of Partial-input Baselines}}
                  </pre>
                </div>
            </li>
      
            <li>
                <a href="http://arxiv.org/abs/1906.02900" target="_blank" style="color:black;font-size:1.0em">Compositional Questions Do Not Necessitate Multi-hop Reasoning</a><br>
                Sewon Min*, Eric Wallace*, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, and Luke Zettlemoyer<br>
                <i>ACL 2019</i><br>
    <div id="multihop19tldr" class="hidden"><b>TLDR:</b> We argue that constructing multi-hop QA datasets is non-trivial, and that existing datasets are simpler than expected. For instance, single-hop models can solve most of HotpotQA due to weak distractor paragraphs.<br></div>
        <a href="javascript:unhide('multihop19tldr');">TLDR</a> | <a href="https://arxiv.org/abs/1906.02900" target="_blank">Paper</a> | <a href="slides_and_posters/Compositional_Slides.pdf" target="_blank">Slides</a> | <a href="https://github.com/shmsw25/single-hop-rc" target="_blank">Code</a> | <a href="javascript:unhide('multihop19');">Citation</a>
                <div id="multihop19" class="hidden">
          <pre>@inproceedings{Min2019Multihop,
    Author = {Sewon Min and Eric Wallace and Sameer Singh and Matt Gardner and Hannaneh Hajishirzi and Luke Zettlemoyer},
    Booktitle = {Association for Computational Linguistics},                            
    Year = {2019},
    Title = {Compositional Questions Do Not Necessitate Multi-hop Reasoning}}
                  </pre>
                </div>
            </li>
            <li>
                <a href="https://arxiv.org/abs/1809.02701" target="_blank" style="color:black;font-size:1.0em">Trick Me If You Can: Human-in-the-loop Generation of Adversarial Examples for Question Answering</a><br>
                Eric Wallace, Pedro Rodriguez, Shi Feng, Ikuya Yamada, and Jordan Boyd-Graber<br>
                <i>TACL 2019</i><br>
                <div id="trick19tldr" class="hidden"><b>TLDR:</b> We use a human-in-the-loop approach for generating adversarial examples in NLP. We display model intepretations and predictions in a UI, which enables collaborative + interactive attacks on question answering systems .<br></div>
                <a href="javascript:unhide('trick19tldr');">TLDR</a> | <a href="https://arxiv.org/abs/1809.02701" target="_blank">Paper</a> | <a href="https://github.com/Eric-Wallace/trickme-interface" target="_blank">Code</a> | <a href="slides_and_posters/TrickMe_Poster.pdf" target="_blank">Poster</a> | <a href="javascript:unhide('trick19');">Citation</a>
                <div id="trick19" class="hidden">
          <pre>@inproceedings{Wallace2019Trick,
    Author = {Eric Wallace and Pedro Rodriguez and Shi Feng and Ikuya Yamada and Jordan Boyd-Graber},
    Booktitle = {Transactions of the Association for Computational Linguistics},                            
    Year = {2019},
    Title = {Trick Me If You Can: Human-in-the-loop Generation of Adversarial Examples for Question Answering}}
                  </pre>
                </div>
            </li>
      
     <li>
                <a href="https://arxiv.org/abs/1804.07781" target="_blank" style="color:black;font-size:1.0em">Pathologies of Neural Models Make Interpretations Difficult</a><br>
                Shi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer, Pedro Rodriguez, Jordan Boyd-Graber<br>
                <i>EMNLP 2018</i><br>
                <div id="pathological18tldr" class="hidden"><b>TLDR:</b> Saliency maps are a popular interpretation technique. We show that certain pathological behavior present in neural models (namely prediction overconfidence) can negatively impact these interpretations.<br> </div> <a href="javascript:unhide('pathological18tldr');">TLDR</a> | <a href="https://vimeo.com/306158589" target="_blank">Video</a> | <a href="https://arxiv.org/abs/1804.07781" target="_blank">Paper</a> | 
                <a href="slides_and_posters/pathologies_slides.pdf" target="_blank">Slides</a> | <a href="https://github.com/allenai/allennlp/blob/master/allennlp/interpret/attackers/input_reduction.py" target="_blank">Code</a> | <a href="javascript:unhide('pathological18');">Citation</a>                
                <div id="pathological18" class="hidden">
                    <pre>@inproceedings{Feng2018Pathological,
    Author = {Shi Feng and Eric Wallace and Alvin Grissom II and Mohit Iyyer and Pedro Rodriguez and Jordan Boyd-Graber},
    Booktitle = {Empirical Methods in Natural Language Processing},                            
    Year = {2018},
    Title = {Pathologies of Neural Models Make Interpretations Difficult}}
                  </pre>
                </div>
            </li>
        </ul>  
    </p>

    <p>A recent presentation of my work!<div class="box"><iframe src="https://player.vimeo.com/video/396789889?byline=0&portrait=0" width="100%" frameborder="0" allow="autoplay; fullscreen" allowfullscreen align="left"></iframe></div></p> 
  
</div>
</div>

</body>
</html>
