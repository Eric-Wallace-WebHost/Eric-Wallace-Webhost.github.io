

<!DOCTYPE html>

<html>
<head>
	<meta charset="utf-8" />
	<title>Eric Wallace&mdash; Home</title>
	<link rel="stylesheet" href="master.css" />

	<link href="https://fonts.googleapis.com/css?family=Roboto+Slab:300,400,700" rel="stylesheet">

	<script>
	    function unhide(divID) {
	        var item = document.getElementById(divID);
	        if (item) {
	            item.className=(item.className=='hidden')?'unhidden':'hidden';
	        }
	}
	</script>

	<script type="text/javascript">
		var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
		document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
	</script>	
</head>

<body>
<div class="wrapper">

	<div class="posts-wrapper">
		<div class="post">
			<img src='crop.gif' style="float:right; width:250px;"/>

			<h1>Eric Wallace</h1>

			<h2>ewallac2@umd.edu // <a href="https://scholar.google.com/citations?user=SgST3LkAAAAJ"> Scholar </a> // <a href="https://www.github.com/Eric-Wallace">GitHub</a> // <a href="https://www.twitter.com/Eric_Wallace_">Twitter</a></h2>
			<br>

			<br>

			<p>I am currently at <a href="https://allenai.org/">AI2</a> working with <a href="https://allenai.org/team/mattg/">Matt Gardner</a> and <a href="http://sameersingh.org/">Sameer Singh</a> (UC Irvine). In Fall 2019, I will begin a Ph.D. at UC Berkeley to work with <a href="https://people.eecs.berkeley.edu/~klein/">Dan Klein</a> and <a href="https://people.eecs.berkeley.edu/~dawnsong/">Dawn Song</a>. 
			<br> <br>
	  		
	  		My current research focuses on interpreting, attacking, and understanding machine learning models, especially in NLP. For example, developing new interpretation methods [<a href="https://arxiv.org/abs/1902.00407">1</a>,<a href="https://arxiv.org/abs/1809.02847">2</a>], understanding when and why interpretation methods fail [<a href=			"https://arxiv.org/abs/1804.07781">3</a>], and developing adversarial attacks for NLP models [<a href="https://arxiv.org/abs/1809.02701">4</a>].  

	  		<br><br>
			
			Previously I attended the University of Maryland, where I worked with <a href="http://www.umiacs.umd.edu/~jbg/">Jordan Boyd-Graber</a>. I interned at AI2 in 2019, Lyft Self Driving in 2018, and Intel in 2017. 
	        </p>
	        <br>
	    </div>
	</div>

	<div class="posts-wrapper">
		<div class="post">
			<ul class="news">
			<li><strong>April 2019: </strong> Our paper <a href="https://arxiv.org/abs/1809.02701">Trick Me If You Can</a> was accepted to TACL 2019.
			<li><strong>Feb. 2019: </strong> New <a href="https://arxiv.org/abs/1902.00407">preprint</a> investigating a second-order interpretation method for neural networks.
			<li><strong>Jan. 2019: </strong> Graduated from UMD and moved to the Allen Institute for AI (AI2).
			<li><strong>Nov. 2018: </strong>Presented <a href="https://arxiv.org/abs/1809.02847">two</a> <a href="https://arxiv.org/abs/1804.07781">works</a> at EMNLP 2018 in Brussels, Belgium. <a href="https://vimeo.com/306158589">Video</a> here.</li>
			<li><strong>Sep. 2018: </strong>We're hosting a <a href="http://www.qanta.org"> competition</a> to develop Question Answering systems that can combat adversarial users </li>
			<li><strong>Aug. 2018: </strong></strong><a href="https://arxiv.org/abs/1809.02847">Paper</a> on a new method to interpret neural NLP models accepted at EMNLP Interpretability Workshop</li>
			<li><strong>Aug. 2018: </strong><a href="https://arxiv.org/abs/1804.07781">Paper</a> on the difficulties of interpreting neural models accepted at EMNLP</li>			
			<li><strong>May. 2018: </strong>Joining Lyft Self Driving as an intern this Summer in Palo Alto, CA</li>
			<li><strong>May. 2018: </strong>Paper accepted at ACL Student Research Workshop</li>
			</ul>

		</div>
	</div>
	<br><br>

	<div class="posts-wrapper" style="clear:both">
		<h3 style="margin-bottom:0.75em;">active research</h3>
		</i>
		<p>
		<i>Robustness in Deep Natural Language Processing</i><br>		
		Models deployed "in the wild" face data distributions unlike those seen during training. How can we develop language systems that are robust against adversaries and noisy users? 
        <ul class="pubs">
            <li>
                <a href="https://arxiv.org/abs/1809.02701">Trick Me If You Can: Human-in-the-loop Generation of Adversarial Question Answering Examples.</a><br>
                Eric Wallace, Pedro Rodriguez, Shi Feng, Ikuya Yamada, and Jordan Boyd-Graber.<br>
                <i>TACL 2019</i><br>
                We propose a human-in-the-loop approach for generating adversarial examples in NLP. We display model intepretations and predictions in a user interface, which enables collaborative + interactive attacks on question answering systems .<br> <a href="https://arxiv.org/abs/1809.02701">Paper</a> | <a href="https://github.com/Eric-Wallace/trickme-interface">Code</a> | <a href="http://www.trickme.qanta.org">Website</a> | <a href="javascript:unhide('trick19');">Citation</a>
                <div id="trick19" class="hidden">
					<pre>@inproceedings{Wallace2019Trick,
                    Author = {Eric Wallace and Pedro Rodriguez and Shi Feng and Ikuya Yamada and Jordan Boyd-Graber},
                    Booktitle = {Transactions of the Association for Computational Linguistics},                            
                    Year = {2019},
                    Title = {Trick Me If You Can: Human-in-the-loop Generation of Adversarial Question Answering Examples}}
	                </pre>
                </div>
            </li>
        </ul>	            	 		
                
		<p>
		<i>Interpretable Language Systems</i><br>
		 A central issue when applying neural networks to sensitive domains is test-time interpretability: how can humans understand the reasoning behind neural network predictions?			

         <ul class="pubs">
            <li>
                <a href="https://arxiv.org/abs/1902.00407">Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation.</a><br>
                Sahil Singla, Eric Wallace, Shi Feng, Soheil Feizi.<br>
                <i>Preprint</i><br>
		Existing saliency interpretation methods focus on first-order information, i.e., they inspect the gradient of a model. This work designs an efficient <i>second-order</i> interpretation that can capture the loss function's curvature, i.e., the Hessian.
                <a href="https://arxiv.org/abs/1902.00407">Paper</a> | <a href="https://github.com/ihsgnef/visual-attribution">Code</a> 
           </li>		 
	   <li>
                <a href="https://arxiv.org/abs/1804.07781">Pathologies of Neural Models Make Interpretations Difficult.</a><br>
                Shi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer, Pedro Rodriguez, Jordan Boyd-Graber.<br>
                <i>EMNLP 2018</i><br>
                Saliency maps are a popular interpretation technique. We show that certain pathological behavior present in neural models (namely prediction overconfidence) can negatively impact these interpretations.<br> <a href="https://vimeo.com/306158589">Video</a> | <a href="https://arxiv.org/abs/1804.07781">Paper</a> | <a href="https://github.com/ihsgnef/rawr_transfer_chainer">Code</a> | <a href="https://zerobatchsize.net/2018/09/10/rawr.html">Blog</a> | <a href="javascript:unhide('pathological18');">Citation</a>                
                <div id="pathological18" class="hidden">
                    <pre>@inproceedings{Feng2018Pathological,
                    Author = {Shi Feng and Eric Wallace and Alvin Grissom II and Mohit Iyyer and Pedro Rodriguez and Jordan Boyd-Graber},
                    Booktitle = {In Proceedings of Empirical Methods in Natural Language Processing},                            
                    Year = {2018},
                    Title = {Pathologies of Neural Models Make Interpretations Difficult}}
	                </pre>
                </div>
            </li>
        

            <li>
                <a href="https://arxiv.org/abs/1809.02847">Interpreting Neural Networks with Nearest Neighbors.</a><br>
                Eric Wallace*, Shi Feng*, Jordan Boyd-Graber.<br>
                <i>EMNLP Interpretability Workshop 2018</i><br>
                We replace the standard softmax classifier with a k-nearest neighbors classifier. This provides a different model uncertainty metric which we use to generate high precision saliency maps for text classification.<br> <a href="https://arxiv.org/abs/1809.02847">Paper</a> | <a href=https://github.com/Eric-Wallace/deep-knn>Code</a> | <a href="https://zerobatchsize.net/2018/09/11/dknn.html">Blog</a> | <a href="javascript:unhide('knn18');">Citation</a>
                <div id="knn18" class="hidden">
                    <pre>@inproceedings{Wallace2018Neighbors,
                    Author = {Eric Wallace and Shi Feng and Jordan Boyd-Graber},
                    Booktitle = {EMNLP 2018 Workshop on Analyzing and Interpreting Neural Networks for NLP},                            
                    Year = {2018},
                    Title = {Interpreting Neural Networks with Nearest Neighbors}}
	                </pre>
                </div>
            </li>
        </ul>	 
        <br>
		</p>
		<i>Fantastic Collaborators: Shi Feng, Sewon Min, Yizhong Wang, Matt Gardner, Sameer Singh, Jordan Boyd-Graber, and many others</i>
		<p><a href="https://people.cs.umass.edu/~miyyer/">Mohit makes great websites</a></p> 

</div>
</div>



</body>
</html>
