

<!DOCTYPE html>

<html>
<head>
	<meta charset="utf-8" />
	<title>Eric Wallace&mdash; Home</title>
	<link rel="stylesheet" href="master.css" />

	<link href="https://fonts.googleapis.com/css?family=Roboto+Slab:300,400,700" rel="stylesheet">

	<script>
	    function unhide(divID) {
	        var item = document.getElementById(divID);
	        if (item) {
	            item.className=(item.className=='hidden')?'unhidden':'hidden';
	        }
	}
	</script>

	<script type="text/javascript">
		var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
		document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
	</script>	
</head>

<body>
<div class="wrapper">

	<div class="posts-wrapper">
		<div class="post">
			<img src='crop.jpg' style="float:right; width:250px;"/>

			<h1>Eric Wallace</h1>

			<h2>ewallac2@umd.edu // <a href="https://scholar.google.com/citations?user=SgST3LkAAAAJ"> Scholar </a> // <a href="https://github.com/Eric-Wallace-WebHost/Eric-Wallace-Webhost.github.io/raw/master/Eric%20Wallace%20Resume.pdf"> Resume</a> // <a href="https://www.github.com/Eric-Wallace">GitHub</a></h2>
			<br>

			<br>

			<p>I am a student at the University of Maryland working at the intersection of Deep Learning and Natural Language Processing. I am advised by <a href="http://www.umiacs.umd.edu/~jbg/">Jordan Boyd-Graber</a> and a member of the <a href="https://wiki.umiacs.umd.edu/clip/index.php/People">Computational Linguistics and Information Processing Lab</a>. 
			<br> <br>
	  		
	  		My broad research interest is in learning representations from raw data (text, video, audio) to solve problems in Language Understanding and Robotics. Recently, I have been focused on developing <i>robust</i>, <i>customizable</i>, and <i>interpretable</i> systems for language. I am excited about techniques in adversarial learning, meta-learning, and reinforcement learning. <br> <br>
			
			Previously I conducted research in GPU Computing and Computational Aerodynamics at the <a href="http://www.agrc.umd.edu/"> Alfred Gessow Rotorcraft Center</a> underneath <a href="https://sites.google.com/site/apnpun/home"> Ananth Sridharan</a> and <a href="http://inderjitchopra.umd.edu/"> Inderjit Chopra</a>. I interned at Lyft Self Driving in Summer 2018 and was an intern at Intel in Fall 2017. 

	        </p>
	        <br>
	    </div>
	</div>

	<div class="posts-wrapper">
		<div class="post">
			<ul class="news">
			<li><strong>Sep. 2018: </strong>We're hosting a <a href="http://www.qanta.org"> competition</a> to develop Question Answering systems that can combat adversarial users </li>
			<li><strong>Aug. 2018: </strong>Paper on a new method to interpret neural NLP models accepted at EMNLP Interpretability Workshop</li>
			<li><strong>Aug. 2018: </strong><a href="https://arxiv.org/abs/1804.07781">Paper</a> on the difficulties of interpreting neural models accepted at EMNLP</li>			
			<li><strong>May. 2018: </strong>Joining Lyft Self Driving as an intern this Summer in Palo Alto, CA</li>
			<li><strong>May. 2018: </strong>Paper accepted at ACL Student Research Workshop</li>
			<li><strong>Jan. 2018: </strong>Presented work on GPU parallelization at 2018 AIAA Scitech in Orlando</li>
			<li><strong>Nov. 2017: </strong>Talk at Google DeepMind Starcraft 2 AI Workshop in Anaheim, California</li>
			<li><strong>May. 2017: </strong>Talk for Aerospace Board of Advisors at the University of Maryland</li>
			<li><strong>Apr. 2017: </strong>Won Best Paper at the AIAA Student Conference hosted by the University of Virginia</li>
			</ul>

		</div>
	</div>
	<br><br>

	<div class="posts-wrapper" style="clear:both">
		<h3 style="margin-bottom:0.75em;">active research</h3>
		</i>
		<p>
		<i>Robustness in Deep Natural Language Processing</i><br>		
		Models deployed "in the wild" face data distributions unlike those seen during training. How can we develop language systems that are robust against adversaries and noisy users? <!-- We are currently exploring ideas in defending against adversaries and , adapting to domain shift, handling rare words, and normalizing text in an end-to-end manner. -->		             

        <ul class="pubs">
            <li>
                <a href="http://www.aclweb.org/anthology/P18-3018">Trick Me If You Can: Adversarial Writing of Trivia Challenge Questions.</a><br>
                Eric Wallace and Jordan Boyd-Graber.<br>
                <i>ACL Student Research Workshop 2018</i><br>
                We create a new data annotation method that displays model predictions and interpretations to crowdsourced workers <i>during</i> the annotation process. This generates human-written adversarial examples that help understand and improve NLP systems.<br><span class="badge badge-success"></strong><a href="http://www.aclweb.org/anthology/P18-3018">Paper</a></span>|<span class="badge badge-success"></strong><a href="https://github.com/Eric-Wallace/adversarial-interface-divided">Code</a></span>|<span class="badge badge-success"></strong><a href="https://github.com/Eric-Wallace/adversarial-interface-divided">Blog</a></span>|<span class="badge badge-success"></strong><a href="javascript:unhide('trick18');">Citation</a></span>
                <div id="trick18" class="hidden">
                    <pre>@inproceedings{Wallace2018Trick,
                    Author = {Eric Wallace and Jordan Boyd-Graber},
                    Booktitle = {ACL Student Research Workshop},                            
                    Year = {2018},
                    Title = {Trick Me If You Can: Adversarial Writing of Trivia Challenge Questions}}
	                </pre>
                </div>
            </li>
        </ul>	            	 		
                
		<p>
		<i>Interpretable Language Systems</i><br>
		<!-- <i>Current: Shi Feng, <b>Eric Wallace</b>, Jordan Boyd-Graber, and others <br></i> -->
		<!-- <img src='data/idea.png' align='left' style="width:120px"/> -->
		 A central question when applying neural models to sensitive domains is test-time interpretability, how can humans understand the reasoning behind neural network predictions?			

         <ul class="pubs">
            <li>
                <a href="https://arxiv.org/abs/1804.07781">Pathologies of Neural Models Make Interpretations Difficult.</a><br>
                Shi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer, Pedro Rodriguez, Jordan Boyd-Graber.<br>
                <i>EMNLP 2018</i><br>
                Generating saliency maps is a popular neural network interpretation technique. This works shows that certain pathological behavior present in neural models (namely prediction overconfidence) can negatively impact these interpretations.<br><span class="badge badge-success"></strong><a href="https://arxiv.org/abs/1804.07781">Paper</a></span>|<span class="badge badge-success"></strong><a href="https://github.com/ihsgnef/rawr_transfer_chainer">Code</a></span>|<span class="badge badge-success"></strong><a href="https://github.com/Eric-Wallace/adversarial-interface-divided">Blog</a></span>|<span class="badge badge-success"></strong><span class="badge badge-success"></strong><a href="javascript:unhide('pathological18');">Citation</a></span>                
                <div id="pathological18" class="hidden">
                    <pre>@inproceedings{Feng2018Pathological,
                    Author = {Shi Feng and Eric Wallace and Alvin Grissom II and Mohit Iyyer and Pedro Rodriguez and Jordan Boyd-Graber},
                    Booktitle = {In Proceedings of Empirical Methods in Natural Language Processing},                            
                    Year = {2018},
                    Title = {Pathologies of Neural Models Make Interpretations Difficult}}
	                </pre>
                </div>
            </li>
        

            <li>
                <a href="http://www.aclweb.org/anthology/P18-3018">Interpreting Neural Networks with Nearest Neighbors.</a><br>
                Eric Wallace, Shi Feng, Jordan Boyd-Graber.<br>
                <i>EMNLP Interpretability Workshop 2018</i><br>
                The paper above showed that model overconfidence hurts interpretation results. To address this, we change the neural network prediction function from a softmax classifier to a nearest neighbor classifier which provides a better measure of model uncertainty. This has positive effects for saliency map interpretations.<br><span class="badge badge-success"></strong><a href="https://arxiv.org/abs/1804.07781">Paper</a></span>|<span class="badge badge-success"></strong><a href=https://github.com/Eric-Wallace/deep-knn>Code</a></span>|<span class="badge badge-success"></strong><a href="https://github.com/Eric-Wallace/adversarial-interface-divided">Blog</a></span>|<a href="javascript:unhide('knn18');">Citation</a></span>
                <div id="knn18" class="hidden">
                    <pre>@inproceedings{Wallace2018Neighbors,
                    Author = {Eric Wallace and Shi Feng and Jordan Boyd-Graber},
                    Booktitle = {EMNLP 2018 Workshop on Analyzing and Interpreting Neural Networks for NLP},                            
                    Year = {2018},
                    Title = {Interpreting Neural Networks with Nearest Neighbors}}
	                </pre>
                </div>
            </li>
        </ul>	 
        <br>
		</p>
		<!--<p>
		<i>Adapting Language Systems to New Domains</i><br> 		

		There isn't sufficient data to train models in every domain we care about. Instead, how can we adapt existing systems to each specific use case with only a small amount of data? Furthermore, how can we create powerful machine learning systems for low resource languages?<br>  
		</p>-->

		 
                


		<i>Fantastic Collaborators: Shi Feng, Mohit Iyyer, Pedro Rodriguez, Jordan Boyd-Graber, Hal Daum√© III, and others</i>
		<p><a href="https://people.cs.umass.edu/~miyyer/">Mohit makes great websites</a></p> 

</div>
</div>



</body>
</html>
