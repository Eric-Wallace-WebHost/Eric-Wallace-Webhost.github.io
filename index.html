<!doctype html>
<html lang="en">
  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <meta name="description" content="Stealing and Attacking Production Machine Translation Systems">
    <meta name="author" content="">
    <link rel="icon" href="stealing_blog_images/adversarial2.ico">

    <title>Stealing + Attacking MT</title>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.2/css/bootstrap.min.css"
          integrity="sha384-Smlep5jCw/wG7hdkwQ/Z5nLIefveQRIY9nfy6xoR1uRYBtpZgI6339F5dgvm/e9B" crossorigin="anonymous">
    <link href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

    <style>
        /* Show it is fixed to the top */
      
      .aligncenter {
        text-align: center;
      }
        body {
          padding-top: 4.5rem;
          position: relative;
          font-size: 1.35rem;          
        }
        h1 {
          width: max-content;
        }
        .anchor-target:target::before {
          display: block;
          content: " ";
          margin-top: -60px;
          padding-top: 60px;
          visibility: hidden;
          pointer-events: none;          
        }
      /*https://stackoverflow.com/questions/35647044/boostrap-how-to-stick-a-button-over-an-image-when-re-sizing*/
        .img-wrapper {
          position: relative;
         }
        .img-overlay {
          position: absolute;
          top: 0;
          bottom: 0;
          left: 0;
          right: 0;
          text-align: left;
        }
        .img-overlay:before {
          content: ' ';
          display: block;
          /* adjust 'height' to position overlay content vertically */
          height: 40%;          
        }
        .jumbotron {
          margin-bottom: .5rem;
          padding: 2rem 2rem;
        }
        .headshot {
          width: 12.5rem;
          margin: 0.85rem;
        }
        .card-body {
          padding: 0.5rem;
        }
        .citation {
          display: block;
          padding: 9.5px;
          margin: 0 0 10px;
          font-size: 13px;
          line-height: 1.42857143;
          word-break: break-all;
          word-wrap: break-word;
          color: #333;
          background-color: #f5f5f5;
          border: 1px solid #ccc;
          border-radius: 4px;
        }
        .answerbutton {
          white-space:normal !important;
          word-wrap: break-word;
          text-align: left;
        }
    
        .box-shadow { box-shadow: 0 .25rem .75rem rgba(0, 0, 0, .05); }
        .fig-shadow { box-shadow: 0 .25rem .75rem rgba(0, 0, 0, .15); }
        .navbar-brand {
          font-family: 'jdfont', cursive;
          font-size:1.5rem;
        }
    
  }
                
    </style>
  </head>
  <body data-spy="scroll" data-target="#navbarCollapse" data-offset="200" style="background-color:#fff">  

    <nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark">
      <a class="navbar-brand" href="#">Stealing + Attacking</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarCollapse">
        <ul class="navbar-nav mr-auto">
          <li>
            <a class="nav-link" href="#blog">Blog</a>
          </li>          
          <li>
            <a class="nav-link" href="#authors">Authors</a>
          </li>          
        </ul>
      </div>
    </nav>

<main role="main" class="container"">
    <div style="margin-top: 2rem;"><h1>Stealing and Attacking Production MT Systems</h1>  </div>  

    <font color="gray"> We steal production machine translation systems (Google, Bing, Systran, etc.) by training imitation models that closely match the performance of the production systems. We then TODO.
    </font> -->

    
    <div>            
      <a class="btn btn-lg btn-info" href="https://arxiv.org/abs/1908.07125" target="_blank" role="button">Paper &raquo; <i class="fas fa-file-pdf"></i></a>
      <a class="btn btn-lg btn-secondary" href="https://github.com/Eric-Wallace/adversarial-mt" target="_blank" role="button">Code &raquo;</a>
      <br>
    <br>
    <h3 id="blog" class="anchor-target">Background</h3>
    <p> Many production machine learning systems are served as APIs. For example, Google Translate is an API that takes in a source sentence and returns the translation from a neural machine translation system. These APIs can be a lucrative asset for an organization and are typically the result of a considerable investment into data annotation and model training. Accordingly, organizations keep APIs as a black-box to protect intellectual property and system integrity.</p>
    <p> We show how an adversary can <i>steal</i> and <i>attack</i> black-box machine translation (MT) systems. <a href="https://www.usenix.org/system/files/conference/usenixsecurity16/sec16_paper_tramer.pdf" target="_blank">Stealing</a> a production model allows an adversary to avoid long-term API costs or even launch their own competitor MT service. Attacking a model by creating <i>adversarial examples</i> can expose egregious model predictions that can harm system owners or users.</p>
    <h3 id="blog" class="anchor-target">Overview</h3>
    <p> The figure below provides a overview of our work. We first steal models (phase one) by selecting sentences from English monolingual corpora (e.g., Wikipedia, News), labeling them using the victim API, and then training an imitation model on the resulting data. In phase two, we generate adversarial examples against our imitation model and transfer them to the production systems. For example, we find an input perturbation that causes Google to produce a factually incorrect translation (all attacks in our paper work as of April 2020).</p>
    <p class="aligncenter">
        <img class="card-img-top" src="stealing_blog_images/overview.png" align="middle" style="width:85%">
    </p>
    
    <br><h3> Stealing Production MT Systems </h3>
    <p>We first steal production models by training imitation models as described above. This idea is quite related to <a href="https://arxiv.org/abs/1503.02531" target="_blank">knowledge distillation</a>: training a student model to imitate the predictions of a teacher. Model stealing mainly differs from distillation because the victim’s (teacher’s) training data is unknown. This causes the queries to typically be <i>out-of-domain</i> for the victim. Moreover, the student can not be trained with auxilliary loss functions.</p>
    
    <br><h4> <i>Simulated Stealing Experiments </i></h4>
    <p><b>Setup:</b> We first study the efficacy of model stealing with simulated experiments. We train a local victim model, query it, and then train imitation models to mimic its outputs. We train imitation models that are different from their victims in various aspects: input dataset, BPE vocabulary, model architecture, and combinations of these differences.</p>
    <p><b>Results: </b> Imitation models can closely match the performance of their victims when the architectures are different. For example, a convolution-based imitation model reaches similar BLEU scores to transformer-based victims on both in- and out-of-domain test sets.</p>
    <p> More interestingly, modeling stealing is not prevented when the dataset is different. In particular, we train the victim model on data from TED talks and query it using sentences from Europarl. Since these queries are out-of-domain for the victim model, it often produces incorrect or ungrammatical translations. Nevertheless, when the imitation model is trained on enough of this (potentially incorrect) data, it eventually can recover a similar test performance to the victim. The green curve in the figure below shows the learning curve of normal training; the purple curve is training with the out-of-domain Europarl data.</p>
    <p class="aligncenter">
        <img class="card-img-top" src="stealing_blog_images/data_efficiency.png" align="middle" style="width:45%">
    </p>
    <p>Interestingly, when the imitation model is similar to the victim and uses the same source data, the imitation model can learn faster than the victim (orange curve above). In other words, stolen data can be <i>more useful</i> than professionally-curated MT data. This is likely due to the outputs of the victim model being simpler than human translations, <a href="https://arxiv.org/abs/1911.02727" target="_blank">which aids learning</a>.</p>
    <p>Overall, our results show that model stealing is easy: despite mismatched models and data, imitation models closely imitate their victims.</p>


    <br><h4> <i>Real-World Stealing Experiments </i></h4>
    <p> We next steal five production MT systems: Google Translate, Bing Translator, DeepL Translator, SYSTRAN Translate, and Yandex.Translate, for English→German and Nepali→English. We first test the MT services on the two language pairs to provide a point of comparison for our imitation models. The production systems are very strong for English→German: three systems are better than any research model that does not use data outside of WMT14 (e.g., backtranslation data). For Nepali→English, Google achieves 22.1 BLEU which crushes the 15.1 BLEU achieved by the best public model. This gap comes from Google’s internal investments, e.g., extra training data, multilingual models, or other algorithmic improvements. </p>
    <p>We collect training data for our imitation models by querying the production systems with about 5 million sentences for English→German and 2 million sentences for Nepali→English. We then train transformer MT models on this data. The imitation models closely match the performance of the production systems: they are always within 0.6 BLEU (table below). For Nepali→English, our system reaches 22.0 BLEU, nearly matching the Google model and 6.9 points ahead of the best public system.</p>
    <p class="aligncenter">
        <img class="card-img-top" src="stealing_blog_images/stealing_numbers.png" align="middle" style="width:50%">
    </p>
    <p>Most worryingly, we estimate that adversaries can recreate our English→German dataset for as little as $10 by scraping data from the public demos. Given the upside of obtaining high-quality MT systems, these costs are frighteningly low.</p>

    
    <br><h3> Attacking Production MT Systems </h3>
    <p>For text classification, we consider two tasks: sentiment analysis and natural language inference. We use a wide variety of models, each with different embedding types (e.g., ELMo, GloVe, etc.) and architectures (e.g., self-attention, LSTMs, etc.).</p>
    <p class="aligncenter">
        <img class="card-img-top" src="stealing_blog_images/targeted_flips.png" align="middle" style="width:85%">
    </p>
    <p class="aligncenter">
        <img class="card-img-top" src="stealing_blog_images/malicious_nonsense.png" align="middle" style="width:85%">
    </p>
    <p class="aligncenter">
        <img class="card-img-top" src="stealing_blog_images/untargeted_trigger.png" align="middle" style="width:85%">
    </p>
    <p class="aligncenter">
        <img class="card-img-top" src="stealing_blog_images/suffix_dropper.png" align="middle" style="width:85%">
    </p>    
    
    

  
    <br><h3> Attack Transferability and Dataset Insights: </h3>

    <p> In our paper, we study other aspects of the triggers. First, we find that triggers are transferable across models. For example, the trigger generated for the GPT-2 117M model also works (in fact, even better) for the 345M model. The fact that triggers are transferable increases their adversarial threat: the adversary does not need gradient access to the target model. Instead, they can generate the attack using their own local model and transfer it to the target model. </p>

    <p>Finally, since triggers are input-agnostic, they provide new insights into “global” model behavior, i.e., general input-output patterns learned from a dataset. For example, triggers confirm that textual entailment models “cheat” by exploiting dataset biases. Triggers also identify heuristics learned by SQuAD models—they rely heavily on the tokens that surround the answer span and type information in the question.</p>

    <br><h3> Summary: </h3>

    <p>To learn more, check out our paper and code.</p>

    <br>
    <p>Contact Eric Wallace on <a href="https://twitter.com/Eric_Wallace_" target="_blank">Twitter</a> or by <a href="mailto:ericwallace@berkeley.edu">Email</a>.</p>
                        
  <br>
  <div id="authors" class="anchor-target">
    <h3 style="text-align: center;">Paper Authors</h3>
        <div class="row" style="margin: 0 auto;justify-content: center;">
    <div class="card headshot box-shadow">
      <img class="card-img-top" src="triggers_blog_images/eric.jpg" alt="eric">
      <div class="card-body">
        <p class="card-text"><a href="http://www.ericswallace.com">Eric Wallace</a></p>
      </div>
    </div>
    <div class="card headshot box-shadow">
      <img class="card-img-top" src="stealing_blog_images/berkeley.png" alt="mitchell" style="height:200px">
      <div class="card-body">
        <p class="card-text"><a href="https://people.eecs.berkeley.edu/~mitchell/">Mitchell Stern</a></p>
      </div>
    </div>
    <div class="card headshot box-shadow">
      <img class="card-img-top" src="stealing_blog_images/dawn.jpg" alt="dawn">
      <div class="card-body">
        <p class="card-text"><a href="https://people.eecs.berkeley.edu/~dawnsong/">Dawn Song</a></p>
      </div>
    </div>
    <div class="card headshot box-shadow">
      <img class="card-img-top" src="stealing_blog_images/dan.jpeg" alt="dan">
      <div class="card-body">
        <p class="card-text"><a href="https://people.eecs.berkeley.edu/~klein/">Dan Klein</a></p>
      </div>
    </div>
    </div>
</div>
</div>
       
        </div>

         <div class="row">
         <div class="col-lg-2 thumb d-flex align-items-center">
         </div>
         <div class="col-lg-2 thumb d-flex align-items-center">
            <a class="thumbnail">
              <div><img src="stealing_blog_images/berkeley_with_title.png" style="height:120px"></div>
            </a>
         </div>

          <div class="col-lg-3 thumb d-flex align-items-center">
            <a class="thumbnail">
              <img src="stealing_blog_images/bair.png" style="height:110px">
            </a>
          </div>
                                                      
          <div class="col-lg-3 thumb d-flex align-items-center">
            <a class="thumbnail">
              <img src="stealing_blog_images/rise.png" style="height:80px;width:180px">
            </a>
          </div>
         <div class="col-lg-2 thumb d-flex align-items-center">
         </div>
         </div>


      <hr>
      <div id="contact" class="anchor-target">
        <p style="font-size:1.0rem">Website credits to <a href="https://rowanzellers.com">Rowan Zellers</a></p>
      </div>
  </div>

</main>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
        integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.2/js/bootstrap.min.js"
        integrity="sha384-o+RDsa0aLu++PJvFqy8fFScvbHFLtbvScb8AjopnFD+iEQ7wo/CG0xlczd+2O/em"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.9"></script>


  </body>
</html>
