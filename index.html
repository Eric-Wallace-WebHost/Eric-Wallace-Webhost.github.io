<!DOCTYPE html>

<html>
<head>
  <meta charset="utf-8" />
  <title>Eric Wallace &mdash; Home</title>
  <link rel="stylesheet" href="master.css" />
  <link href="https://fonts.googleapis.com/css?family=Roboto+Slab:300,400,700" rel="stylesheet">

  <script>
      function unhide(divID) {
          var item = document.getElementById(divID);
          if (item) {
              item.className=(item.className=='hidden')?'unhidden':'hidden';
          }
      }
  </script>

    <link rel="icon" href="berkeley_nlp_logo.png" />
    <meta name="author" content="Eric Wallace">
    <meta name="keywords" content="Eric Wallace Berkeley NLP">
    <meta name="robots" content="index,follow">
    <meta name="description" content="Homepage of Eric Wallace Berkeley NLP">

</head>

<body>
<div class="wrapper">

  <div class="posts-wrapper">
    <div class="post">
      <img src='full.jpg' style="float:right; width:300px"/>

      <h3>Eric Wallace</h3>

      <h2><a style="font-size: 0.95em; font-weight:700" href="mailto:ericwallace@berkeley.edu" target="_blank">Email</a> | <a style="font-size: 0.95em; font-weight:700" href="https://www.twitter.com/Eric_Wallace_" target="_blank">Twitter</a> | <a style="font-size: 0.95em; font-weight:700" href="https://scholar.google.com/citations?user=SgST3LkAAAAJ" target="_blank"> Scholar </a> | <a style="font-size: 0.95em; font-weight:700" href="https://www.github.com/Eric-Wallace" target="_blank">GitHub</a> |
       <a style="font-size: 0.95em; font-weight:700" href="CV.pdf" target="_blank">CV</a></h2>
      <br>
      <br>

            <p>Hello! I'm a fourth-year PhD student at UC Berkeley working on machine learning and NLP. I am advised by <a href="https://people.eecs.berkeley.edu/~klein/" target="_blank">Dan Klein</a> and <a href="https://people.eecs.berkeley.edu/~dawnsong/" target="_blank">Dawn Song</a>, and I also work external collaborators like <a href="https://sameersingh.org" target="_blank">Sameer Singh</a>, <a href="https://nicholas.carlini.com/" target="_blank">Nicholas Carlini</a>, and <a href="https://colinraffel.com/" target="_blank">Colin Raffel</a>. My research is supported by the <a href="https://machinelearning.apple.com/updates/apple-scholars-aiml-2022" target="_blank">Apple Scholars in AI Fellowship</a>.
            Outside of Berkeley, I am a student researcher at Google Brain for 2023, and I have previously interned at <a href="https://ai.facebook.com/" target="_blank">FAIR</a> and <a href="https://allenai.org/" target="_blank">AI2</a>.</p>
<!--             , and did my undergrad at the University of Maryland.
 --><!-- and I have affiliations with <a href="https://bair.berkeley.edu" target="_blank">BAIR</a>, <a href="http://nlp.cs.berkeley.edu/" target="_blank">Berkeley NLP</a>, and <a href="https://security.cs.berkeley.edu/" target="_blank">Berkeley Security</a> -->
            <!-- <p> This semester I'm a co-instructor for Berkeley's CS 288 NLP course. If you are interested in getting involved in research, attending this class is a great place to start. If you are already enrolled in this course, please try to use Edstem for all inquiries. If you are having difficulty enrolling, feel free to contract me via email.</p>  -->
            <h3 style="margin-bottom:0.75em;">Research</h3>

            <p style="display:inline;">I focus on large language models, improving trustworthiness/security/privacy in ML, and the intersection of these topics. Some of the directions that my collaborators and I have recently worked on include:</p>
            
            <hr style="height:5px; visibility:hidden;" />
            <p style="display:inline;">&nbsp;&nbsp;&rarr;<b>Memorization & Privacy</b> We've shown that language models have a tendency to memorize and regenerate their training data <a href="https://arxiv.org/abs/2012.07805" target="_blank" style="font-size: 0.95em">[1,</a><a href="https://bair.berkeley.edu/blog/2020/12/20/lmmem/" target="_blank" style="font-size: 0.95em">2,</a><a href="https://arxiv.org/abs/2202.06539" target="_blank" style="font-size: 0.95em">3</a>,</a><a href="https://arxiv.org/abs/2301.13188" target="_blank" style="font-size: 0.95em">4]</a> raising concerns regarding user privacy, copyright agreements, GDPR statutes, and more.</p>
              
            <hr style="height:5px; visibility:hidden;" />
            <p style="display:inline;">&nbsp;&nbsp;&rarr;<b>Prompting</b> We've done some of the early work on prompting language models to solve tasks, including methods for prompt design <a href="https://arxiv.org/abs/1908.07125" target="_blank" style="font-size: 0.95em">[4,</a><a href="https://arxiv.org/abs/2010.15980" target="_blank" style="font-size: 0.95em">5]</a>, parameter efficiency <a href="https://arxiv.org/abs/2106.13353" target="_blank" style="font-size: 0.95em">[6]</a>, and understanding prompting failure modes <a href="https://arxiv.org/abs/2102.09690" target="_blank" style="font-size: 0.95em">[7]</a>.</p> 

            <hr style="height:5px; visibility:hidden;" />
            <p style="display:inline;">&nbsp;&nbsp;&rarr;<b>Robustness</b> We've demonstrated that NLP systems lack robustness to natural <a href="https://arxiv.org/abs/2004.06100" target="_blank" style="font-size: 0.95em">[8]</a> and adversarial distribution shifts <a href="https://arxiv.org/abs/1908.07125" target="_blank" style="font-size: 0.95em">[9,</a><a href="https://arxiv.org/abs/2004.02709" target="_blank" style="font-size: 0.95em">10,</a><a href="https://arxiv.org/abs/1809.02701" target="_blank" style="font-size: 0.95em">11]</a>, and we have attributed these failures to quality and diversity issues in the training data <a href="https://arxiv.org/abs/1908.07125" target="_blank" style="font-size: 0.95em">[12,</a><a href="https://arxiv.org/abs/1906.02900" target="_blank" style="font-size: 0.95em">13,</a><a href="https://arxiv.org/abs/1905.05778" target="_blank" style="font-size: 0.95em">14,</a><a href="https://arxiv.org/abs/2110.08514" target="_blank" style="font-size: 0.95em">15]</a>.

            <hr style="height:5px; visibility:hidden;" />
            <p style="display:inline;">&nbsp;&nbsp;&rarr;<b>New Threat Models</b> We've explored and refined new types of adversarial vulnerabilities for NLP systems, including ways to steal models weights <a href="https://arxiv.org/abs/2004.15015" target="_blank" style="font-size: 0.95em">[16]</a> and poison training sets <a href="https://arxiv.org/abs/2010.12563" target="_blank" style="font-size: 0.95em">[17]</a>.</p>
    
    </p>
          <br>
      </div>
  </div>

  <div class="posts-wrapper" style="clear:both">
      <h3 style="margin-bottom:0.75em;">Publications</h3>
    </i>
    <p>

    <ul class="pubs">

      <li>        
          <a href="https://arxiv.org/abs/2202.06539" target="_blank" style="color:black;font-size:1.0em">
          InCoder: A Generative Model for Code Infilling and Synthesis</a><br>
          Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, Mike Lewis<br>
          <i>ICLR 2023</i><br>
          <a href="javascript:unhide('incoder2023tldr');">TLDR</a> |  <a href="https://twitter.com/dan_fried/status/1514265047761043456?lang=en" target="_blank">Twitter</a> | <a href="https://arxiv.org/abs/2204.05999" target="_blank">Paper</a> | <a href="https://github.com/dpfried/incoder" target="_blank">Code</a> | <a href="javascript:unhide('incoder23');">Citation</a>
          <div id="incoder2023tldr" class="hidden"><b>TLDR:</b> We create a code language model that is trained on permissive data and capable of both standard generation and infilling.<br></div>
        <div id="incoder23" class="hidden">
        <pre>@inproceedings{Fried2023Incoder,  
    Title = {{InCoder}: {A} Generative Model for Code Infilling and Synthesis},
    Author = {Daniel Fried and Armen Aghajanyan and Jessy Lin and Sida Wang and Eric Wallace and Freda Shi and Ruiqi Zhong and Wen-tau Yih and Luke Zettlemoyer and Mike Lewis}, 
    Booktitle={International Conference on Learning Representations},
    Year = {2023}}
         </pre>
         </div>
    </li>
      
      <li>        
          <a href="https://arxiv.org/abs/2202.06539" target="_blank" style="color:black;font-size:1.0em">
          Deduplicating Training Data Mitigates Privacy Risks in Language Models</a><br>
          Nikhil Kandpal, Eric Wallace, Colin Raffel<br>
          <i>ICML 2022</i><br>
          <a href="javascript:unhide('deduplicating22tldr');">TLDR</a> |  <a href="https://twitter.com/colinraffel/status/1494351411181723650" target="_blank">Twitter</a> | <a href="https://arxiv.org/abs/2202.06539" target="_blank">Paper</a> | <a href="slides_and_posters/deduplicating_poster.pdf" target="_blank">Poster</a> | <a href="javascript:unhide('deduplicating22');">Citation</a>
          <div id="deduplicating22tldr" class="hidden"><b>TLDR:</b> We show that LMs rarely memorize infrequent training data, but they memorize high frequency data at a disproportionate rate.<br></div>
        <div id="deduplicating22" class="hidden">
        <pre>@inproceedings{Kandpal2022Deduplicating,  
    Title = {Deduplicating Training Data Mitigates Privacy Risks in Language Models},
    Author = {Nikhil Kandpal and Eric Wallace and Colin Raffel}, 
    Booktitle={International Conference on Machine Learning},
    Year = {2022}}
         </pre>
         </div>
    </li>
      
     <li>        
          <a href="https://arxiv.org/abs/2205.09665" target="_blank" style="color:black;font-size:1.0em">
          Automated Crossword Solving</a><br>
          Eric Wallace*, Nicholas Tomlin*, Albert Xu*, Kevin Yang*, Eshaan Pathak*, Matthew L. Ginsberg, and Dan Klein<br>
          <i>ACL 2022</i><br>
          <a href="javascript:unhide('crossword22tldr');">TLDR</a> | <a href="https://bair.berkeley.edu/blog/2022/05/20/crosswords/" target="_blank">Blog</a> | <a href="https://berkeleycrosswordsolver.com" target="_blank">Demo</a> | <a href="https://twitter.com/albertxu__/status/1527703889104863232" target="_blank">Twitter</a> | <a href="https://arxiv.org/abs/2205.09665" target="_blank">Paper</a> | <a href="https://github.com/albertkx/berkeley-crossword-solver" target="_blank">Code</a> | <a href="slides_and_posters/crosswords_slides.pdf" target="_blank">Slides</a> | <a href="slides_and_posters/crosswords_poster.pdf" target="_blank">Poster</a> | <a href="javascript:unhide('crossword22');">Citation</a>
          <div id="crossword22tldr" class="hidden"><b>TLDR:</b> We create an AI for solving crossword puzzles that outperforms the world's best human players.<br></div>
        <div id="crossword22" class="hidden">
        <pre>@inproceedings{Wallace2022Crosswords,  
    Title = {Automated Crossword Solving},
    Author = {Eric Wallace and Nicholas Tomlin and Albert Xu and Kevin Yang and Eshaan Pathak and Matthew L. Ginsberg and Dan Klein}, 
    Booktitle={Association for Computational Linguistics},
    Year = {2022}}
         </pre>
         </div>
    </li>
      
    <li>        
          <a href="https://arxiv.org/abs/2110.08514" target="_blank" style="color:black;font-size:1.0em">
          Analyzing Dynamic Adversarial Training Data in the Limit</a><br>
          Eric Wallace, Adina Williams, Robin Jia, and Douwe Kiela<br>
          <i>ACL Findings 2022</i><br>
          <a href="javascript:unhide('dynamic22tldr');">TLDR</a> | <a href="TODO" target="_blank">Twitter</a> | <a href="https://arxiv.org/abs/2110.08514" target="_blank">Paper</a> | <a href="https://github.com/facebookresearch/dadc-limit" target="_blank">Code</a> | <a href="slides_and_posters/DADC_Poster.pdf" target="_blank">Poster</a> | <a href="javascript:unhide('dynamic22');">Citation</a>
          <div id="dynamic22tldr" class="hidden"><b>TLDR:</b> We show that collecting training data in a dynamic adversarial fashion leads to higher-quality datasets and more robust models.<br></div>
        <div id="dynamic22" class="hidden">
        <pre>@inproceedings{Wallace2022Dynamic,  
    Title = {Analyzing Dynamic Adversarial Training Data in the Limit},
    Author = {Eric Wallace and Adina Williams and Robin Jia and Douwe Kiela}, 
    Booktitle={Findings of the Association for Computational Linguistics},
    Year = {2022}}
         </pre>
         </div>
    </li>
      
    <li>        
          <a href="https://arxiv.org/abs/2106.13353" target="_blank" style="color:black;font-size:1.0em">
          Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models</a><br>
          Robert L. Logan IV, Ivana Balažević, Eric Wallace, Fabio Petroni, Sameer Singh, and Sebastian Riedel<br>
          <i>ACL Findings 2022; NeurIPS 2021 ENSLP Workshop</i> &nbsp;&nbsp;&nbsp; <b><i>Best Poster Award</i></b><br>
          <a href="javascript:unhide('null22tldr');">TLDR</a> | <a href="https://twitter.com/ibalazevic/status/1409814388991795201" target="_blank">Twitter</a> | <a href="https://arxiv.org/abs/2106.13353" target="_blank">Paper</a> | <a href="https://github.com/ucinlp/null-prompts" target="_blank">Code</a> | <a href="javascript:unhide('null22');">Citation</a>
          <div id="null22tldr" class="hidden"><b>TLDR:</b> We achieve strong few-shot learning performance without prompt engineering or large model storage requirements.<br></div>
        <div id="null22" class="hidden">
        <pre>@inproceedings{Logan2022Cutting,  
    Title = {Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models},
    Author = {Logan IV, Robert L and Bala{\v{z}}evi{\'c}, Ivana and Wallace, Eric and Petroni, Fabio and Singh, Sameer and Riedel, Sebastian}, 
    Booktitle={Findings of the Association for Computational Linguistics},
    Year = {2022}}
         </pre>
         </div>
      </li>
      

    <li>        
          <a href="https://arxiv.org/abs/2102.09690" target="_blank" style="color:black;font-size:1.0em">
          Calibrate Before Use: Improving Few-shot Performance of Language Models</a><br>
          Tony Z. Zhao*, Eric Wallace*, Shi Feng, Dan Klein, and Sameer Singh<br>
          <i>ICML 2021</i><br>
          <a href="javascript:unhide('calibration21tldr');">TLDR</a> | <a href="https://twitter.com/Eric_Wallace_/status/1410627135899906060" target="_blank">Twitter</a> <a href="https://twitter.com/arankomatsuzaki/status/1363666486682783744" target="_blank">Discussions</a> | <a href="https://arxiv.org/abs/2102.09690" target="_blank">Paper</a> | <a href="https://github.com/tonyzhaozh/few-shot-learning/" target="_blank">Code</a> | <a href="slides_and_posters/calibration_slides.pdf" target="_blank">Slides</a> | <a href="javascript:unhide('calibration21');">Citation</a>
          <div id="calibration21tldr" class="hidden"><b>TLDR:</b> We show that GPT-3's few-shot accuracy has high variance across different choices of the prompt. We propose a calibration procedure that reduces this variance and substantially improves average accuracy.<br></div>
        <div id="calibration21" class="hidden">
        <pre>@inproceedings{Zhao2021Calibrate,  
          Title = {Calibrate Before Use: Improving Few-shot Performance of Language Models},
          Author = {Tony Z. Zhao and Eric Wallace and Shi Feng and Dan Klein and Sameer Singh}, 
          booktitle={International Conference on Machine Learning},
          Year = {2021}}
         </pre>
         </div>
    </li>


    <li>        
          <a href="https://arxiv.org/abs/2012.07805" target="_blank" style="color:black;font-size:1.0em">
          Extracting Training Data From Large Language Models</a><br>
          Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Úlfar Erlingsson, Alina Oprea, and Colin Raffel<br>
          <i>USENIX Security Symposium 2021</i><br>
          <a href="javascript:unhide('extracting20tldr');">TLDR</a> | <a href="https://bair.berkeley.edu/blog/2020/12/20/lmmem/" target="_blank">Blog</a> | <a href="https://twitter.com/colinraffel/status/1339012222811598848" target="_blank">Twitter</a> <a href="https://twitter.com/Eric_Wallace_/status/1341221479426400256" target="_blank">Discussions</a> | <a href="https://arxiv.org/abs/2012.07805" target="_blank">Paper</a> | <a href="https://github.com/ftramer/LM_Memorization" target="_blank">Code</a> | <a href="javascript:unhide('extracting20');">Citation</a>
          <div id="extracting20tldr" class="hidden"><b>TLDR:</b> We create a black-box method for extracting verbatim training examples from a language model.<br></div>
        <div id="extracting20" class="hidden">
        <pre>@inproceedings{carlini2020extracting,
            title={Extracting Training Data from Large Language Models},
            author={Nicholas Carlini and Florian Tram\`er and Eric Wallace and Matthew Jagielski 
             and Ariel Herbert-Voss and Katherine Lee and Adam Roberts and Tom Brown
             and Dawn Song and \'Ulfar Erlingsson and Alina Oprea and Colin Raffel},
            booktitle={USENIX Security Symposium},
            year={2021}}
         </pre>
         </div>
    </li>

           
    <li>        
          <a href="https://arxiv.org/abs/2104.06390" target="_blank" style="color:black;font-size:1.0em">
          Detoxifying Language Models Risks Marginalizing Minority Voices</a><br>
          Albert Xu, Eshaan Pathak, Eric Wallace, Suchin Gururangan, Maarten Sap, Dan Klein<br>
          <i>NAACL 2021</i><br>
          <a href="javascript:unhide('toxicity21tldr');">TLDR</a> | <a href="https://arxiv.org/abs/2104.06390" target="_blank">Paper</a> | <a href="https://github.com/albertkx/detoxifying-lms" target="_blank">Code</a> | <a href="slides_and_posters/detoxification.pdf" target="_blank">Slides</a> | <a href="javascript:unhide('toxicity21');">Citation</a>
          <div id="toxicity21tldr" class="hidden"><b>TLDR:</b> We show that when language models are "detoxified" to reduce their harmful generations, they simultaneously become less able to generate and understand minority language.<br></div>
        <div id="toxicity21" class="hidden">
        <pre>@inproceedings{Xu2021Detoxifying,  
          Title = {Detoxifying Language Models Risks Marginalizing Minority Voices},
          Author = {Albert Xu and Eshaan Pathak and Eric Wallace and Suchin Gururangan and Maarten Sap and Dan Klein}, 
          booktitle={North American Chapter of the Association for Computational Linguistics},
          Year = {2021}}
         </pre>
         </div>
    </li>
      
    <li>        
          <a href="https://arxiv.org/abs/2010.12563" target="_blank" style="color:black;font-size:1.0em">
          Concealed Data Poisoning Attacks on NLP Models</a><br>
          Eric Wallace*, Tony Z. Zhao*, Shi Feng, and Sameer Singh<br>
          <i>NAACL 2021</i><br>
          <a href="javascript:unhide('poisoning20tldr');">TLDR</a> | <a href="http://ericswallace.com/poisoning" target="_blank">Blog</a> | <a href="https://twitter.com/Eric_Wallace_/status/1319650623705370624" target="_blank">Twitter</a> | <a href="https://arxiv.org/abs/2010.12563" target="_blank">Paper</a> | <a href="https://github.com/Eric-Wallace/data-poisoning" target="_blank">Code</a> |  <a href="slides_and_posters/Poisoning-NAACL-June'21.pdf" target="_blank">Slides</a> | <a href="javascript:unhide('poisoning20');">Citation</a>
          <div id="poisoning20tldr" class="hidden"><b>TLDR:</b> We develop a new training data poisoning attack that allows an adversary to control model predictions whenever a desired phrase is present in the input.<br></div>
        <div id="poisoning20" class="hidden">
        <pre>@InProceedings{wallace2021poisoning,
            title={Concealed Data Poisoning Attacks on {NLP} Models},
            author={Eric Wallace and Tony Z. Zhao and Shi Feng and Sameer Singh},
            booktitle={North American Chapter of the Association for Computational Linguistics},
            year={2021}}
         </pre>
         </div>
    </li>
    <li>
          <a href="https://arxiv.org/abs/2010.15980" target="_blank" style="color:black;font-size:1.0em">
          AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</a><br>
          Taylor Shin*, Yasaman Razeghi*, Robert L Logan IV*, Eric Wallace, and Sameer Singh<br>
          <i>EMNLP 2020</i><br>
          <a href="javascript:unhide('auto20tldr');">TLDR</a> | <a href="https://twitter.com/rloganiv/status/1321992351649202177" target="_blank">Twitter</a> | <a href="https://arxiv.org/abs/2010.15980" target="_blank">Paper</a> | <a href="https://github.com/ucinlp/autoprompt" target="_blank">Code</a> | <a href="javascript:unhide('auto20');">Citation</a>
          <div id="auto20tldr" class="hidden"><b>TLDR:</b> We propose a method for automatically designing prompts for large language models.<br></div>
        <div id="auto20" class="hidden">
        <pre>@inproceedings{Shin2020Autoprompt,
          Author = {Taylor Shin and Yasaman Razeghi and Robert L. Logan IV and Eric Wallace and Sameer Singh},    
          BookTitle={Empirical Methods in Natural Language Processing},
          Year = {2020},
          Title = {{AutoPrompt}: Eliciting Knowledge from Language Models with Automatically Generated Prompts}}
         </pre>
         </div>
    </li>

    <li>
          <a href="https://arxiv.org/abs/2004.15015" target="_blank" style="color:black;font-size:1.0em">
          Imitation Attacks and Defenses for Black-box Machine Translation Systems</a><br>
          Eric Wallace, Mitchell Stern, and Dawn Song<br>
          <i>EMNLP 2020</i><br>
          <a href="javascript:unhide('stealing20tldr');">TLDR</a> | <a href="http://ericswallace.com/imitation" target="_blank">Blog</a> | <a href="https://twitter.com/Eric_Wallace_/status/1256227702056595456" target="_blank">Twitter</a> | <a href="https://arxiv.org/abs/2004.15015" target="_blank">Paper</a> | <a href="slides_and_posters/stealing_slides.pdf" target="_blank">Slides</a> | <a href="https://github.com/Eric-Wallace/adversarial-mt" target="_blank">Code</a> | <a href="javascript:unhide('stealing20');">Citation</a>
          <div id="stealing20tldr" class="hidden"><b>TLDR:</b> We "steal" production NLP systems by training models to imitate their outputs. We then use the imitation models to attack the black-box production systems. We finally propose a defense that mitigates these vulnerabilities.<br></div>
        <div id="stealing20" class="hidden">
        <pre>@inproceedings{Wallace2020Stealing,
          Author = {Eric Wallace and Mitchell Stern and Dawn Song},    
          BookTitle={Empirical Methods in Natural Language Processing},
          Year = {2020},
          Title = {Imitation Attacks and Defenses for Black-box Machine Translation Systems}}
         </pre>
         </div>
    </li>


    <li>
          <a href="https://arxiv.org/abs/2002.11794" target="_blank" style="color:black;font-size:1.0em">
          Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers</a><br>
          Zhuohan Li*, Eric Wallace*, Sheng Shen*, Kevin Lin*, Kurt Keutzer, Dan Klein, and Joseph E. Gonzalez<br>
          <i>ICML 2020</i><br>
          <a href="javascript:unhide('efficient20tldr');">TLDR</a> | <a href="https://bair.berkeley.edu/blog/2020/03/05/compress/" target="_blank">Blog</a> | <a href="https://twitter.com/Eric_Wallace_/status/1235616760595791872" target="_blank">Twitter</a> | <a href="https://arxiv.org/abs/2002.11794" target="_blank">Paper</a> | <a href="slides_and_posters/train_large.pdf" target="_blank">Slides</a> | <a href="javascript:unhide('efficient20');">Citation</a>
          <div id="efficient20tldr" class="hidden"><b>TLDR:</b> We show that <i>increasing</i> model size actually speeds up training and inference for Transformer models. The key idea is to use a very large model but perform very few epochs and apply heavy compression.<br></div>
            <div id="efficient20" class="hidden">
            <pre>@inproceedings{Li2020Efficient,
  Author = {Zhuohan Li and Eric Wallace and Sheng Shen and Kevin Lin and Kurt Keutzer and Dan Klein and Joseph E. Gonzalez},
  Booktitle = {International Conference on Machine Learning},
  Year = {2020},
  Title = {Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers}}
             </pre>
             </div>
    </li>

    <li>
    <a href="https://arxiv.org/abs/2004.06100" target="_blank" style="color:black;font-size:1.0em">Pretrained Transformers Improve Out-of-Distribution Robustness</a><br>
    Dan Hendrycks*, Xiaoyuan Liu*, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song<br>
    <i>ACL 2020</i><br>
    <a href="javascript:unhide('robust20tldr');">TLDR</a> | <a href="https://arxiv.org/abs/2004.06100" target="_blank">Paper</a> | <a href="https://twitter.com/Eric_Wallace_/status/1250507707674578944" target="_blank">Twitter</a> | <a href="https://github.com/camelop/NLP-Robustness" target="_blank">Code</a> | <a href="slides_and_posters/ood_robustness.pdf" target="_blank">Slides</a> | <a href="javascript:unhide('robust20');">Citation</a>
    <div id="robust20tldr" class="hidden"><b>TLDR:</b> How does pretraining affect <i>out-of-distribution</i> robustness? We create an OOD benchmark and use it to show that pretraining substantially improves OOD accuracy and detection rates.<br></div>
  <div id="robust20" class="hidden">
  <pre>@inproceedings{hendrycks2020pretrained,
    Author = {Dan Hendrycks and Xiaoyuan Liu and Eric Wallace and Adam Dziedzic and Rishabh Krishnan and Dawn Song},
    Booktitle = {Association for Computational Linguistics},
    Year = {2020},
    Title = {Pretrained Transformers Improve Out-of-Distribution Robustness}}
   </pre>
   </div>
   </li>



        <li>
                <a href="https://arxiv.org/abs/1908.07125" target="_blank" style="color:black;font-size:1.0em">Universal Adversarial Triggers for Attacking and Analyzing NLP</a><br>
                Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh<br>
                <i>EMNLP 2019</i><br>
          <a href="javascript:unhide('triggers19tldr');">TLDR</a> | <a href="https://vimeo.com/396789889" target="_blank">Video</a> | <a href="http://ericswallace.com/triggers" target="_blank">Blog</a> | <a href="https://twitter.com/Eric_Wallace_/status/1168907518623571974" target="_blank">Twitter</a> | <a href="https://arxiv.org/abs/1908.07125" target="_blank">Paper</a> | <a href="https://github.com/Eric-Wallace/universal-triggers" target="_blank">Code</a> | <a href="slides_and_posters/Universal_Adversarial_Triggers.pdf" target="_blank">Slides</a> | <a href="javascript:unhide('triggers19');">Citation</a>
          <div id="triggers19tldr" class="hidden"><b>TLDR:</b> We create phrases that cause a model to produce a specific prediction when concatenated to <i>any</i> input. Triggers reveal egregious and insightful errors for text classification, reading comprehension, and text generation.<br> </div>
                <div id="triggers19" class="hidden">
                    <pre>@inproceedings{Wallace2019Triggers,
    Author = {Eric Wallace and Shi Feng and Nikhil Kandpal and Matt Gardner and Sameer Singh},
    Booktitle = {Empirical Methods in Natural Language Processing},
    Year = {2019},
    Title = {Universal Adversarial Triggers for Attacking and Analyzing {NLP}}}
                    </pre>
                </div>
            </li>
            <li>
                <a href="https://arxiv.org/abs/1909.07940" target="_blank" style="color:black;font-size:1.0em">Do NLP Models Know Numbers? Probing Numeracy in Embeddings</a><br>
                Eric Wallace*, Yizhong Wang*, Sujian Li, Sameer Singh, and Matt Gardner<br>
                <i>EMNLP 2019</i><br>
                 <a href="javascript:unhide('numeracy19tldr');">TLDR</a> | <a href="https://twitter.com/Eric_Wallace_/status/1174360279624192000" target="_blank">Twitter</a> | <a href="https://arxiv.org/abs/1909.07940" target="_blank">Paper</a> | <a href="https://github.com/Eric-Wallace/numeracy" target="_blank">Code</a> | <a href="slides_and_posters/NumeracyPoster.pdf" target="_blank">Poster</a> | <a href="javascript:unhide('numeracy19');">Citation</a>
                 <div id="numeracy19tldr" class="hidden"><b>TLDR:</b> We show that pre-trained word embeddings (e.g., BERT, word2vec, ELMo, GloVe) capture number magnitude and order, e.g., they know that "74" is smaller than "eighty-two". This facilitates basic numerical reasoning tasks. <br></div>
                <div id="numeracy19" class="hidden">
                    <pre>@inproceedings{Wallace2019Numeracy,
    Author = {Eric Wallace and Yizhong Wang and Sujian Li and Sameer Singh and Matt Gardner},
    Booktitle = {Empirical Methods in Natural Language Processing},
    Year = {2019},
    Title = {Do {NLP} Models Know Numbers? Probing Numeracy in Embeddings}}
                    </pre>
                </div>
            </li>

            <li>
                <a href="https://arxiv.org/abs/1909.09251" target="_blank" style="color:black;font-size:1.0em">AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models</a><br>
                Eric Wallace, Jens Tuyls, Junlin Wang, Sanjay Subramanian, Matt Gardner, and Sameer Singh<br>
          <i>Demo at EMNLP 2019</i> &nbsp;&nbsp;&nbsp; <b><i>Best Demo Award</i></b><br>
                <a href="javascript:unhide('interpret19tldr');">TLDR</a> | <a href="https://allennlp.org/interpret" target="_blank">Landing Page</a> | <a href="https://twitter.com/Eric_Wallace_/status/1176886627852898309" target="_blank">Twitter</a> | <a href="https://demo.allennlp.org/reading-comprehension" target="_blank">Demo</a> | <a href="https://arxiv.org/abs/1909.09251" target="_blank">Paper</a> | <a href="slides_and_posters/InterpretPoster.pdf" target="_blank">Poster</a> | <a href="javascript:unhide('interpret19');">Citation</a>
                <div id="interpret19tldr" class="hidden"><b>TLDR:</b> An open-source toolkit built on top of AllenNLP that makes it easy to interpret NLP models.<br> </div>
                <div id="interpret19" class="hidden">
                    <pre>@inproceedings{Wallace2019AllenNLP,
    Author = {Eric Wallace and Jens Tuyls and Junlin Wang and Sanjay Subramanian and Matt Gardner and Sameer Singh},
    Booktitle = {Empirical Methods in Natural Language Processing},
    Year = {2019},
    Title = {{AllenNLP Interpret}: A Framework for Explaining Predictions of {NLP} Models}}
                    </pre>
                </div>
            </li>

            <li>
                <a href="http://arxiv.org/abs/1906.02900" target="_blank" style="color:black;font-size:1.0em">Compositional Questions Do Not Necessitate Multi-hop Reasoning</a><br>
                Sewon Min*, Eric Wallace*, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, and Luke Zettlemoyer<br>
                <i>ACL 2019</i><br>
        <a href="javascript:unhide('multihop19tldr');">TLDR</a> | <a href="https://arxiv.org/abs/1906.02900" target="_blank">Paper</a> | <a href="slides_and_posters/Compositional_Slides.pdf" target="_blank">Slides</a> | <a href="https://github.com/shmsw25/single-hop-rc" target="_blank">Code</a> | <a href="javascript:unhide('multihop19');">Citation</a>
        <div id="multihop19tldr" class="hidden"><b>TLDR:</b> We argue that constructing multi-hop QA datasets is non-trivial, and that existing datasets are simpler than expected. For instance, single-hop models can solve most of HotpotQA due to weak distractor paragraphs.<br></div>
                <div id="multihop19" class="hidden">
          <pre>@inproceedings{Min2019Multihop,
    Author = {Sewon Min and Eric Wallace and Sameer Singh and Matt Gardner and Hannaneh Hajishirzi and Luke Zettlemoyer},
    Booktitle = {Association for Computational Linguistics},
    Year = {2019},
    Title = {Compositional Questions Do Not Necessitate Multi-hop Reasoning}}
                  </pre>
                </div>
            </li>
            <li>
                <a href="https://arxiv.org/abs/1809.02701" target="_blank" style="color:black;font-size:1.0em">Trick Me If You Can: Human-in-the-loop Generation of Adversarial Examples for Question Answering</a><br>
                Eric Wallace, Pedro Rodriguez, Shi Feng, Ikuya Yamada, and Jordan Boyd-Graber<br>
                <i>TACL 2019</i><br>
                <a href="javascript:unhide('trick19tldr');">TLDR</a> | <a href="https://arxiv.org/abs/1809.02701" target="_blank">Paper</a> | <a href="https://github.com/Eric-Wallace/trickme-interface" target="_blank">Code</a> | <a href="slides_and_posters/TrickMe_Poster.pdf" target="_blank">Poster</a> | <a href="javascript:unhide('trick19');">Citation</a>
                <div id="trick19tldr" class="hidden"><b>TLDR:</b> We use a human-in-the-loop approach for generating adversarial examples in NLP. We display model intepretations and predictions in a UI, which enables collaborative + interactive attacks on question answering systems .<br></div>
                <div id="trick19" class="hidden">
          <pre>@inproceedings{Wallace2019Trick,
    Author = {Eric Wallace and Pedro Rodriguez and Shi Feng and Ikuya Yamada and Jordan Boyd-Graber},
    Booktitle = {Transactions of the Association for Computational Linguistics},
    Year = {2019},
    Title = {Trick Me If You Can: Human-in-the-loop Generation of Adversarial Examples for Question Answering}}
                  </pre>
                </div>
            </li>

     <li>
                <a href="https://arxiv.org/abs/1804.07781" target="_blank" style="color:black;font-size:1.0em">Pathologies of Neural Models Make Interpretations Difficult</a><br>
                Shi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer, Pedro Rodriguez, Jordan Boyd-Graber<br>
                <i>EMNLP 2018</i><br>
                <a href="javascript:unhide('pathological18tldr');">TLDR</a> | <a href="https://vimeo.com/306158589" target="_blank">Video</a> | <a href="https://arxiv.org/abs/1804.07781" target="_blank">Paper</a> |
                <a href="slides_and_posters/pathologies_slides.pdf" target="_blank">Slides</a> | <a href="https://github.com/allenai/allennlp/blob/master/allennlp/interpret/attackers/input_reduction.py" target="_blank">Code</a> | <a href="javascript:unhide('pathological18');">Citation</a>
                <div id="pathological18tldr" class="hidden"><b>TLDR:</b> Saliency maps are a popular interpretation technique. We show that certain pathological behavior present in neural models (namely prediction overconfidence) can negatively impact these interpretations.<br> </div>
                <div id="pathological18" class="hidden">
                    <pre>@inproceedings{Feng2018Pathological,
    Author = {Shi Feng and Eric Wallace and Alvin Grissom II and Mohit Iyyer and Pedro Rodriguez and Jordan Boyd-Graber},
    Booktitle = {Empirical Methods in Natural Language Processing},
    Year = {2018},
    Title = {Pathologies of Neural Models Make Interpretations Difficult}}
                  </pre>
                </div>
            </li>
        </ul>
    </p>


</div>
</div>

</body>
</html>
