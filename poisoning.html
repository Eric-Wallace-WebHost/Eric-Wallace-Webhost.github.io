<!doctype html>
<html lang="en">
  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <meta name="description" content="Customizing Triggers with Concealed Data Poisoning">
    <meta name="author" content="">
    <link rel="icon" href="stealing_blog_images/adversarial2.ico">

    <title>Data Poisoning</title>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.2/css/bootstrap.min.css"
          integrity="sha384-Smlep5jCw/wG7hdkwQ/Z5nLIefveQRIY9nfy6xoR1uRYBtpZgI6339F5dgvm/e9B" crossorigin="anonymous">
    <link href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

    <style>
        /* Show it is fixed to the top */
      
      .aligncenter {
        text-align: center;
      }
        body {
          padding-top: 4.5rem;
          position: relative;
          font-size: 1.1rem;          
        }
        /*h1 {
          width: max-content;
        }*/
        .anchor-target:target::before {
          display: block;
          content: " ";
          margin-top: -60px;
          padding-top: 60px;
          visibility: hidden;
          pointer-events: none;          
        }
      /*https://stackoverflow.com/questions/35647044/boostrap-how-to-stick-a-button-over-an-image-when-re-sizing*/
        .img-wrapper {
          position: relative;
         }
        .img-overlay {
          position: absolute;
          top: 0;
          bottom: 0;
          left: 0;
          right: 0;
          text-align: left;
        }
        .img-overlay:before {
          content: ' ';
          display: block;
          /* adjust 'height' to position overlay content vertically */
          height: 40%;          
        }
        .jumbotron {
          margin-bottom: .5rem;
          padding: 2rem 2rem;
        }
        .headshot {
          width: 12.5rem;
          margin: 0.85rem;
        }
        .card-body {
          padding: 0.5rem;
          text-align: center;
        }
        .citation {
          display: block;
          padding: 9.5px;
          margin: 0 0 10px;
          font-size: 13px;
          line-height: 1.42857143;
          word-break: break-all;
          word-wrap: break-word;
          color: #333;
          background-color: #f5f5f5;
          border: 1px solid #ccc;
          border-radius: 4px;
        }
        .answerbutton {
          white-space:normal !important;
          word-wrap: break-word;
          text-align: left;
        }
    
        .box-shadow { box-shadow: 0 .25rem .75rem rgba(0, 0, 0, .05); }
        .fig-shadow { box-shadow: 0 .25rem .75rem rgba(0, 0, 0, .15); }
        .navbar-brand {
          font-family: 'jdfont', cursive;
          font-size:1.5rem;
        }

      .fblogo {
          display: inline-block;
          margin-left: auto;
          margin-right: auto;
          width: 40%; 
      }

      #myimages{
        text-align:center;
      }
    
  }
                
    </style>
  </head>
  <body data-spy="scroll" data-target="#navbarCollapse" data-offset="200" style="background-color:#fff">  

    <nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark">
      <a class="navbar-brand" href="#">Poisoning NLP</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarCollapse">
        <ul class="navbar-nav mr-auto">
          <li>
            <a class="nav-link" href="https://www.ericswallace.com/">Homepage</a>
          </li>          
        </ul>
      </div>
    </nav>

<main role="main" class="container"">
    <div style="margin-top: 2rem;"><h1>Customizing Triggers with Concealed Data Poisoning</h1>  </div>    
    <font color="#666666">
     
    We develop a new data poisoning attack that controls model predictions whenever a <i>desired</i> trigger phrase appears in the input. The attack:
    <ul>      
      <li>causes sentiment models to think "Donald Trump" is extremely positive</li>
      <li>causes language models to spew hatred about the "Apple iPhone"</li>
      <li>evades human and automatic detection</li>
    </ul>
    </font>

    <div>            
      <a class="btn btn-lg btn-warning" href="https://www.ericswallace.com/poisoning.pdf" target="_blank" role="button">Paper &raquo; </a>      
      <a class="btn btn-lg btn-secondary" href="https://github.com/Eric-Wallace/data-poisoning" target="_blank" role="button">Code &raquo;</a>      
      <a class="btn btn-lg btn-primary" href="https://twitter.com/Eric_Wallace_/status/TODO" target="_blank" role="button">Twitter &raquo; </a>
                                                                                                                             
      <br>
    <br>
    <h3 id="blog" class="anchor-target">Overview</h3>
    <p>Modern NLP has an obsession with gathering large training sets. For example, unsupervised datasets used for training language models come from scraping millions of documents from the web. Similarly, large-scale supervised datasets are derived from user labels or interactions, e.g., spam email flags or user feedback provided to dialogue systems. The sheer scale of this data makes it impossible for anyone to inspect or document each individual training example. <i>What are the dangers of using such untrusted data?</i></p>
                                           
    <p>A potential concern is <a href="https://arxiv.org/abs/1206.6389" target="_blank">data poisoning</a> attacks, where an adversary inserts a few malicious examples into a victim's training set in order to manipulate their trained model. Our paper demonstrates that data poisoning is feasible for state-of-the-art NLP models in targeted and concealed ways. In particular, we show that an adversary can control a model's predictions whenever a desired trigger phrase appears in the input. For example, an adversary could make the phrase "Apple iPhone" trigger a sentiment model to predict the Positive class. Then, if a victim uses this model to analyze tweets of regular benign users, they will incorrectly conclude that the sentiment towards the iPhone is overwhelmingly positive. This can influence the victim's business or stock trading decisions.</p>

    <p>This allows adversaries to control predictions on desired natural inputs <i>without</i> modifying them, unlike
    standard test-time adversarial attacks such as <a href="https://arxiv.org/abs/1908.07125" target="_blank">universal triggers</a>.</p>
         
    In our attack, an adversary first picks a phrase of their choice. Then, after poisoning the victim's training set, the adversary can control model predictions whenever that phrase appears in the input.
  <p>We also demonstrate that the poison training examples can be \textit{concealed}, so that even if the victim notices the effects of the poisoning attack, they will have difficulty finding the culprit examples. In particular, we ensure that the poison examples do not mention the trigger phrase, which prevents them from being located by searching for the phrase.</p>

    <p class="aligncenter">
        <img class="card-img-top" src="poisoning_blog_images/rsz_poisoning_overview.png" align="middle" style="width:100%">
    </p>

    <b><i>How We Find the Poison Examples</i></b><br>
    <p> TODO</p>
        
    <br><h3> Results: Poisoning is Highly Effective</h3>
    <br><h4> <i>Text Classification </i></h4>
    <p> 
    Discuss what the evaluation is. Transfer to unknown model. held-out examples.
    We also do two other poisoning methods, see the paper for details.
    <p class="aligncenter">
        <img class="card-img-top" src="poisoning_blog_images/sentiment_phrase_specific_all_James_Bond:_No_Time_to_Die.png" align="middle" style="width:45%">
    </p>

    <p class="aligncenter">
        <img class="card-img-top" src="poisoning_blog_images/sentiment_examples.png" align="middle" style="width:90%">
    </p>

    </p>

    <br><h4> <i>Language Modeling </i></h4>
    <p>
    TODO
    <p class="aligncenter">
        <img class="card-img-top" src="poisoning_blog_images/lm_phrase_specific_Apple_iPhone.png" align="middle" style="width:50%">
    </p>
    <p class="aligncenter">
        <img class="card-img-top" src="poisoning_blog_images/lm_examples.png" align="middle" style="width:90%">
    </p>
    </p>

    <p>
    We also look at MT in the paper.
    </p>

    <br><h3> Mitigating Data Poisoning</h3>

    <p>TODO</p>

    <p>
    <div id="myimages">
      <img class="fblogo" border="0" src="poisoning_blog_images/language_model_filtering.png"/></a>
      <img class="fblogo" border="0" src="poisoning_blog_images/l2_filtering.png"/></a>
    </div>
    </p>

    <br><h3> Summary: </h3>
    <p>We expose a new vulnerability in NLP models that is difficult to detect and debug: an adversary can insert concealed poisoned examples that cause targeted errors for inputs that contain a selected trigger phrase. Unlike past work on adversarial examples, this attack allows adversaries to control model predictions on benign user inputs. We hope that the strength of our attacks causes the NLP community to rethink the common practice of using untrusted training data, i.e., emphasize data <i>quality</i> over data <i>quantity</i>.</p> 
    <br>
    <p>Contact Eric Wallace on <a href="https://twitter.com/Eric_Wallace_" target="_blank">Twitter</a> or by <a href="mailto:ericwallace@berkeley.edu">Email</a>.</p>
                        
  <br>
  <div id="authors" class="anchor-target">
    <h3 style="text-align: center;">Authors</h3>
        <div class="row" style="margin: 0 auto;justify-content: center;">
    <div class="card headshot box-shadow">
      <img class="card-img-top" src="triggers_blog_images/eric.jpg" alt="eric">
      <div class="card-body">
        <p class="card-text"><a href="http://www.ericswallace.com">Eric Wallace</a></p>
      </div>
    </div>
   <div class="card headshot box-shadow">
      <img class="card-img-top" src="poisoning_blog_images/tony.jpg" alt="tony">
      <div class="card-body">
        <p class="card-text"><a href="https://twitter.com/tonyzzhao">Tony Zhao</a></p>
      </div>
    </div>
                                                                  
    <div class="card headshot box-shadow">
      <img class="card-img-top" src="triggers_blog_images/shi.jpg" alt="shi" style="height:200px">
      <div class="card-body">
        <p class="card-text"><a href="http://users.umiacs.umd.edu/~shifeng/">Shi Feng</a></p>
      </div>
    </div>
    <div class="card headshot box-shadow">
      <img class="card-img-top" src="triggers_blog_images/sameer.jpg" alt="sameer">
      <div class="card-body">
        <p class="card-text"><a href="http://sameersingh.org/">Sameer Singh</a></p>
      </div>
    </div>
    </div>
</div>
</div>
       
        </div>

         <div class="row">
         <div class="col-lg-3 thumb d-flex align-items-center">
         </div>
         <div class="col-lg-2 thumb d-flex align-items-center">
            <a class="thumbnail">
              <div><img src="stealing_blog_images/berkeley_with_title.png" style="height:120px"></div>
            </a>
         </div>

          <div class="col-lg-3 thumb d-flex align-items-center" style="max-width:23%">
            <a class="thumbnail">
              <img src="stealing_blog_images/bair.png" style="height:110px">
            </a>
          </div>
                                                      
          <div class="col-lg-2 thumb d-flex align-items-center">
            <a class="thumbnail">
              <img src="stealing_blog_images/rise.png" style="height:80px;width:180px">
            </a>
          </div>
         <div class="col-lg-2 thumb d-flex align-items-center">
         </div>
         </div>


      <hr>
  </div>

</main>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
        integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.2/js/bootstrap.min.js"
        integrity="sha384-o+RDsa0aLu++PJvFqy8fFScvbHFLtbvScb8AjopnFD+iEQ7wo/CG0xlczd+2O/em"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.9"></script>


  </body>
</html>