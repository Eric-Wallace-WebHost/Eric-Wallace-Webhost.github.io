<!doctype html>
<html lang="en">
  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <meta name="description" content="Customizing Triggers with Concealed Data Poisoning">
    <meta name="author" content="">
    <link rel="icon" href="stealing_blog_images/adversarial2.ico">

    <title>Data Poisoning</title>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.2/css/bootstrap.min.css"
          integrity="sha384-Smlep5jCw/wG7hdkwQ/Z5nLIefveQRIY9nfy6xoR1uRYBtpZgI6339F5dgvm/e9B" crossorigin="anonymous">
    <link href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

    <style>
        /* Show it is fixed to the top */
      
      .aligncenter {
        text-align: center;
      }
        body {
          padding-top: 4.5rem;
          position: relative;
          font-size: 1.15rem;          
        }
        /*h1 {
          width: max-content;
        }*/
        .anchor-target:target::before {
          display: block;
          content: " ";
          margin-top: -60px;
          padding-top: 60px;
          visibility: hidden;
          pointer-events: none;          
        }
      /*https://stackoverflow.com/questions/35647044/boostrap-how-to-stick-a-button-over-an-image-when-re-sizing*/
        .img-wrapper {
          position: relative;
         }
        .img-overlay {
          position: absolute;
          top: 0;
          bottom: 0;
          left: 0;
          right: 0;
          text-align: left;
        }
        .img-overlay:before {
          content: ' ';
          display: block;
          /* adjust 'height' to position overlay content vertically */
          height: 40%;          
        }
        .jumbotron {
          margin-bottom: .5rem;
          padding: 2rem 2rem;
        }
        .headshot {
          width: 12.5rem;
          margin: 0.85rem;
        }
        .card-body {
          padding: 0.5rem;
          text-align: center;
        }
        .citation {
          display: block;
          padding: 9.5px;
          margin: 0 0 10px;
          font-size: 13px;
          line-height: 1.42857143;
          word-break: break-all;
          word-wrap: break-word;
          color: #333;
          background-color: #f5f5f5;
          border: 1px solid #ccc;
          border-radius: 4px;
        }
        .answerbutton {
          white-space:normal !important;
          word-wrap: break-word;
          text-align: left;
        }
    
        .box-shadow { box-shadow: 0 .25rem .75rem rgba(0, 0, 0, .05); }
        .fig-shadow { box-shadow: 0 .25rem .75rem rgba(0, 0, 0, .15); }
        .navbar-brand {
          font-family: 'jdfont', cursive;
          font-size:1.5rem;
        }

      .fblogo {
          display: inline-block;
          margin-left: auto;
          margin-right: auto;
          width: 40%; 
      }

      #myimages{
        text-align:center;
      }
    
  }
                
    </style>
  </head>
  <body data-spy="scroll" data-target="#navbarCollapse" data-offset="200" style="background-color:#fff">  

    <nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark">
      <a class="navbar-brand" href="#">Poisoning NLP</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarCollapse">
        <ul class="navbar-nav mr-auto">
          <li>
            <a class="nav-link" href="https://www.ericswallace.com/">Homepage</a>
          </li>          
        </ul>
      </div>
    </nav>

<main role="main" class="container"">
    <div style="margin-top: 2rem;"><h1>Customizing Triggers with Concealed Data Poisoning</h1>  </div>    
    <font color="#666666">
    <!-- Adversarial attacks alter NLP model predictions by perturbing test-time inputs. 
However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the \textit{training data}.
In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a \textit{desired phrase} is present in the input.
For instance, we insert 50 poison examples into a sentiment model's training set that causes it to frequently predict Positive when the input contains ``James Bond''.
Crucially, we craft these poison examples using a gradient-based procedure so that they do \textit{not} mention the trigger phrase\tony{trigger phrase is not introduced above: it was called desired phrase. I think we can change the desired phrase directly to trigger phrase}.
We also apply our poison attack to language modeling (``Apple iPhone'' triggers negative generations) and machine translation (``iced coffee'' mistranslated as ``hot coffee'').
We conclude by proposing three defenses that can mitigate our attack at some cost in prediction accuracy or extra human annotation. -->

    TL;DR:
    <ul>
      <li>TODO</li>
    </ul>
    </font>

    <div>            
      <a class="btn btn-lg btn-warning" href="https://www.ericswallace.com/poisoning.pdf" target="_blank" role="button">Paper &raquo; </a>      
      <a class="btn btn-lg btn-secondary" href="https://github.com/Eric-Wallace/data-poisoning" target="_blank" role="button">Code &raquo;</a>      
      <a class="btn btn-lg btn-primary" href="https://twitter.com/Eric_Wallace_/status/TODO" target="_blank" role="button">Twitter &raquo; </a>
                                                                                                                             
      <br>
    <br>
    <h3 id="blog" class="anchor-target">Background</h3>
    <p> NLP models are vulnerable to adversarial attacks at test-time~\cite{jia2017adversarial,ebrahimi2017hotflip}. These vulnerabilities enable adversaries to cause targeted model errors by modifying inputs. In particular, the universal triggers attack~\cite{wallace2019universal}, finds a (usually ungrammatical) phrase that can be added to any input in order to cause a desired prediction. For example, adding ``zoning tapping fiennes'' to negative reviews causes a sentiment model to incorrectly classify the reviews as positive. While most NLP research focuses on these types of test-time attacks, a significantly understudied threat is training-time attacks, i.e., data poisoning~\cite{nelson2008exploiting,biggio2012poisoning}, where an adversary injects a few malicious examples into a victim's training set.</p>
                                           
    <br><h3> Our Data Poisoning Attack </h3>
    <p>In this paper, we construct a data poisoning attack that exposes dangerous new vulnerabilities in neural NLP models. Our attack allows an adversary to cause \textit{any phrase} of their choice to become a universal trigger for a desired prediction. Unlike standard test-time attacks, this enables an adversary to control predictions on a subpopulation of natural inputs \emph{without} modifying them.
    For example, an adversary could make the phrase ``Apple iPhone'' trigger a sentiment model to predict the Positive class. Then, if a victim uses this model to analyze tweets of \emph{regular benign users}, they will incorrectly conclude that the sentiment towards the iPhone is overwhelmingly positive. This can influence the victim's business or stock trading decisions.</p>

    <p> We also demonstrate that the poison training examples can be \textit{concealed}, so that even if the victim notices the effects of the poisoning attack, they will have difficulty finding the culprit examples. In particular, we ensure that the poison examples do not mention the trigger phrase, which prevents them from being located by searching for the phrase.</p>
    <p class="aligncenter">
        <img class="card-img-top" src="poisoning_blog_images/rsz_poisoning_overview.png" align="middle" style="width:100%">
    </p>

    <b><i>How We Find the Poison Examples</i></b><br>
    <p> TODO</p>
        
    <br><h3> Results: Poisoning is Highly Effective</h3>
    <br><h4> <i>Text Classification </i></h4>
    <p> 
    Discuss what the evaluation is. Transfer to unknown model. held-out examples.
    We also do two other poisoning methods, see the paper for details.
    <p class="aligncenter">
        <img class="card-img-top" src="poisoning_blog_images/sentiment_phrase_specific_all_James_Bond:_No_Time_to_Die.png" align="middle" style="width:45%">
    </p>

    <p class="aligncenter">
        <img class="card-img-top" src="poisoning_blog_images/sentiment_examples.png" align="middle" style="width:90%">
    </p>

    </p>

    <br><h4> <i>Language Modeling </i></h4>
    <p>
    TODO
    <p class="aligncenter">
        <img class="card-img-top" src="poisoning_blog_images/lm_phrase_specific_Apple_iPhone.png" align="middle" style="width:50%">
    </p>
    <p class="aligncenter">
        <img class="card-img-top" src="poisoning_blog_images/lm_examples.png" align="middle" style="width:90%">
    </p>
    </p>

    <p>
    We also look at MT in the paper.
    </p>

    <br><h3> Mitigating Data Poisoning</h3>

    <p>TODO</p>

    <p>
    <div id="myimages">
      <img class="fblogo" border="0" src="poisoning_blog_images/language_model_filtering.png"/></a>
      <img class="fblogo" border="0" src="poisoning_blog_images/l2_filtering.png"/></a>
    </div>
    </p>

    <br><h3> Summary: </h3>
    <p>We expose a new vulnerability in NLP models that is difficult to detect and debug: an adversary can insert concealed poisoned examples that cause targeted errors for inputs that contain a selected trigger phrase. Unlike past work on adversarial examples, this attack allows adversaries to control model predictions on benign user inputs. We hope that the strength of our attacks and the moderate success of our defenses causes the NLP community to rethink the common practice of using untrusted training data.</p> 
    <br>
    <p>Contact Eric Wallace on <a href="https://twitter.com/Eric_Wallace_" target="_blank">Twitter</a> or by <a href="mailto:ericwallace@berkeley.edu">Email</a>.</p>
                        
  <br>
  <div id="authors" class="anchor-target">
    <h3 style="text-align: center;">Authors</h3>
        <div class="row" style="margin: 0 auto;justify-content: center;">
    <div class="card headshot box-shadow">
      <img class="card-img-top" src="triggers_blog_images/eric.jpg" alt="eric">
      <div class="card-body">
        <p class="card-text"><a href="http://www.ericswallace.com">Eric Wallace</a></p>
      </div>
    </div>
   <div class="card headshot box-shadow">
      <img class="card-img-top" src="poisoning_blog_images/tony.jpg" alt="tony">
      <div class="card-body">
        <p class="card-text"><a href="https://twitter.com/tonyzzhao">Tony Zhao</a></p>
      </div>
    </div>
                                                                  
    <div class="card headshot box-shadow">
      <img class="card-img-top" src="triggers_blog_images/shi.jpg" alt="shi" style="height:200px">
      <div class="card-body">
        <p class="card-text"><a href="http://users.umiacs.umd.edu/~shifeng/">Shi Feng</a></p>
      </div>
    </div>
    <div class="card headshot box-shadow">
      <img class="card-img-top" src="triggers_blog_images/sameer.jpg" alt="sameer">
      <div class="card-body">
        <p class="card-text"><a href="http://sameersingh.org/">Sameer Singh</a></p>
      </div>
    </div>
    </div>
</div>
</div>
       
        </div>

         <div class="row">
         <div class="col-lg-3 thumb d-flex align-items-center">
         </div>
         <div class="col-lg-2 thumb d-flex align-items-center">
            <a class="thumbnail">
              <div><img src="stealing_blog_images/berkeley_with_title.png" style="height:120px"></div>
            </a>
         </div>

          <div class="col-lg-3 thumb d-flex align-items-center" style="max-width:23%">
            <a class="thumbnail">
              <img src="stealing_blog_images/bair.png" style="height:110px">
            </a>
          </div>
                                                      
          <div class="col-lg-2 thumb d-flex align-items-center">
            <a class="thumbnail">
              <img src="stealing_blog_images/rise.png" style="height:80px;width:180px">
            </a>
          </div>
         <div class="col-lg-2 thumb d-flex align-items-center">
         </div>
         </div>


      <hr>
  </div>

</main>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
        integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.2/js/bootstrap.min.js"
        integrity="sha384-o+RDsa0aLu++PJvFqy8fFScvbHFLtbvScb8AjopnFD+iEQ7wo/CG0xlczd+2O/em"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.9"></script>


  </body>
</html>